# java 基础 01-互联网java工程师面试突击（第三季）-中华石杉

- [java 基础 01-互联网java工程师面试突击（第三季）-中华石杉](#java-基础-01-互联网java工程师面试突击第三季-中华石杉)
  - [java 取模与取余的区别](#java-取模与取余的区别)
  - [你知道HashMap底层的数据结构是什么吗?](#你知道hashmap底层的数据结构是什么吗)
  - [JDK 1.8中对hash算法和寻址算法是如何优化的呢?](#jdk-18中对hash算法和寻址算法是如何优化的呢)
  - [你知道HashMap是如何解决hash碰撞问题的吗？](#你知道hashmap是如何解决hash碰撞问题的吗)
  - [说说HashMap是如何进行扩容的可以吗？](#说说hashmap是如何进行扩容的可以吗)
  - [说说synchronized关键字的底层原理是什么？](#说说synchronized关键字的底层原理是什么)
  - [能聊聊你对CAS的理解以及其底层实现原理可以吗 ?](#能聊聊你对cas的理解以及其底层实现原理可以吗-)
  - [ConcurrentHashMap实现线程安全的底层原理到底是什么？](#concurrenthashmap实现线程安全的底层原理到底是什么)
  - [你对JDK中的AQS理解吗？AQS的实现原理是什么？](#你对jdk中的aqs理解吗aqs的实现原理是什么)
  - [说说线程池的底层工作原理可以吗？](#说说线程池的底层工作原理可以吗)
  - [说说线程池的核心配置参数都是干什么的？平时我们应该怎么用？](#说说线程池的核心配置参数都是干什么的平时我们应该怎么用)
  - [如果线上机器突然宕机，线程池的阻塞队列中的请求怎么办？](#如果线上机器突然宕机线程池的阻塞队列中的请求怎么办)
  - [谈谈你对Java内存模型的理解可以吗？](#谈谈你对java内存模型的理解可以吗)
  - [你知道Java内存模型中的原子性、有序性、可见性是什么吗？](#你知道java内存模型中的原子性有序性可见性是什么吗)
  - [能从Java底层角度聊聊volatile关键字的原理吗？](#能从java底层角度聊聊volatile关键字的原理吗)
  - [你知道指令重排以及happens-before原则是什么吗？](#你知道指令重排以及happens-before原则是什么吗)
  - [volatile底层是如何基于内存屏障保证可见性和有序性的？](#volatile底层是如何基于内存屏障保证可见性和有序性的)
  - [说说你对Spring的IOC机制和AOP机制的理解可以吗？](#说说你对spring的ioc机制和aop机制的理解可以吗)
  - [说说你对Spring的AOP机制的理解可以吗？](#说说你对spring的aop机制的理解可以吗)
  - [了解过cglib动态代理吗？他跟jdk动态代理的区别是什么？](#了解过cglib动态代理吗他跟jdk动态代理的区别是什么)
  - [能说说Spring中的Bean是线程安全的吗？](#能说说spring中的bean是线程安全的吗)
  - [30、Spring的事务实现原理是什么？能聊聊你对事务传播机制的理解吗？](#30spring的事务实现原理是什么能聊聊你对事务传播机制的理解吗)
  - [31、能画一张图说说Spring Boot的核心架构吗？](#31能画一张图说说spring-boot的核心架构吗)
  - [32、额外加餐：能画一张图说说Spring的核心架构吗？](#32额外加餐能画一张图说说spring的核心架构吗)
  - [33、能说说Spring中都使用了哪些设计模式吗？](#33能说说spring中都使用了哪些设计模式吗)
  - [34、额外加餐：能画一张图说说Spring Web MVC的核心架构吗？](#34额外加餐能画一张图说说spring-web-mvc的核心架构吗)
  - [35、额外加餐：能画一张图说说Spring Cloud的核心架构吗？](#35额外加餐能画一张图说说spring-cloud的核心架构吗)
  - [36、JVM中有哪几块内存区域？Java 8之后对内存分代做了什么改进？](#36jvm中有哪几块内存区域java-8之后对内存分代做了什么改进)
  - [37、你知道JVM是如何运行起来的吗？我们的对象是如何分配的？](#37你知道jvm是如何运行起来的吗我们的对象是如何分配的)
  - [38、说说JVM在哪些情况下会触发垃圾回收可以吗？](#38说说jvm在哪些情况下会触发垃圾回收可以吗)
  - [39、说说JVM的年轻代垃圾回收算法？对象什么时候转移到老年代？](#39说说jvm的年轻代垃圾回收算法对象什么时候转移到老年代)
  - [40、说说老年代的垃圾回收算法？常用的垃圾回收器都有什么？](#40说说老年代的垃圾回收算法常用的垃圾回收器都有什么)
  - [41、你们生产环境中的Tomat是如何设置JVM参数的？如何检查JVM运行情况？](#41你们生产环境中的tomat是如何设置jvm参数的如何检查jvm运行情况)
  - [42、你在实际项目中是否做过JVM GC优化，怎么做的？](#42你在实际项目中是否做过jvm-gc优化怎么做的)
  - [43、你知道发生OOM之后，应该如何排查和处理线上系统的OOM问题？](#43你知道发生oom之后应该如何排查和处理线上系统的oom问题)
  - [44-46_ 你能聊聊TCP/IP四层网络模型吗？OSI七层网络模型也说一下！(上 中 下)](#44-46_-你能聊聊tcpip四层网络模型吗osi七层网络模型也说一下上-中-下)
  - [47-48、浏览器请求www.baidu.com的全过程大概是怎么样的？（上 下）](#47-48浏览器请求wwwbaiducom的全过程大概是怎么样的上-下)
  - [49、画一下TCP三次握手流程图？为啥是三次而不是二次或者四次呢？](#49画一下tcp三次握手流程图为啥是三次而不是二次或者四次呢)
  - [50、聊聊HTTP协议的工作原理！](#50聊聊http协议的工作原理)
  - [51、聊聊HTTPS的工作原理？为啥用HTTPS就可以加密通信？](#51聊聊https的工作原理为啥用https就可以加密通信)
  - [53-54、MySQL、MyISAM和InnoDB存储引擎的区别是啥？（上 下）](#53-54mysqlmyisam和innodb存储引擎的区别是啥上-下)
  - [55-56、聊聊MySQL的索引实现原理？各种索引你们平时都怎么用的？（上下）](#55-56聊聊mysql的索引实现原理各种索引你们平时都怎么用的上下)
  - [57-58、你能说说事务的几个特性是啥？有哪几种隔离级别？（上下）](#57-58你能说说事务的几个特性是啥有哪几种隔离级别上下)
  - [59、你能说说MySQL数据库锁的实现原理吗？如果死锁了咋办](#59你能说说mysql数据库锁的实现原理吗如果死锁了咋办)
  - [60、MySQL的SQL调优一般都有哪些手段？你们一般怎么做](#60mysql的sql调优一般都有哪些手段你们一般怎么做)
  - [61、聊聊Socket的工作原理？Socket跟TCP IP之间是啥关系？](#61聊聊socket的工作原理socket跟tcp-ip之间是啥关系)
  - [62、进程间是如何通信的？线程间又如何切换呢？](#62进程间是如何通信的线程间又如何切换呢)
  - [63-64、你能聊聊BIO、NIO、AIO分别都是啥？有什么区别？（上 下）](#63-64你能聊聊bionioaio分别都是啥有什么区别上-下)
  - [65、线上服务器CPU 100%了！该怎么排查、定位和解决？](#65线上服务器cpu-100了该怎么排查定位和解决)
  - [66、线上机器的一个进程用kill命令杀不死该怎么办？磁盘空间快满了又该怎么处理？](#66线上机器的一个进程用kill命令杀不死该怎么办磁盘空间快满了又该怎么处理)
  - [67、再谈原子性：Java规范规定所有变量写操作都是原子的](#67再谈原子性java规范规定所有变量写操作都是原子的)
  - [68、32位Java虚拟机中的long和double变量写操作为何不是原子的？](#6832位java虚拟机中的long和double变量写操作为何不是原子的)
  - [69、volatile原来还可以保证long和double变量写操作的原子性](#69volatile原来还可以保证long和double变量写操作的原子性)
  - [70、到底有哪些操作在Java规范中是不保证原子性的呢？](#70到底有哪些操作在java规范中是不保证原子性的呢)
  - [71-72、可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器（上 下）](#71-72可见性涉及的底层硬件概念寄存器高速缓存写缓冲器上-下)
  - [73、深入探秘有序性：Java程序运行过程中发生指令重排的几个地方](#73深入探秘有序性java程序运行过程中发生指令重排的几个地方)
  - [74、JIT编译器对创建对象的指令重排以及double check单例实践](#74jit编译器对创建对象的指令重排以及double-check单例实践)
  - [75、现代处理器为了提升性能的指令乱序和猜测执行的机制！](#75现代处理器为了提升性能的指令乱序和猜测执行的机制)
  - [76、高速缓存和写缓冲器的内存重排序造成的视觉假象](#76高速缓存和写缓冲器的内存重排序造成的视觉假象)
  - [76、高速缓存和写缓冲器的内存重排序造成的视觉假象](#76高速缓存和写缓冲器的内存重排序造成的视觉假象-1)
  - [77、synchronized锁同时对原子性、可见性以及有序性的保证](#77synchronized锁同时对原子性可见性以及有序性的保证)
  - [78、深入分析synchronized是如何通过加锁保证原子性的？](#78深入分析synchronized是如何通过加锁保证原子性的)
  - [79、synchronized是如何使用内存屏障保证可见性和有序性的？](#79synchronized是如何使用内存屏障保证可见性和有序性的)
  - [80、再看volatile关键字对原子性、可见性以及有序性的保证](#80再看volatile关键字对原子性可见性以及有序性的保证)
  - [81-82、高速缓存的数据结构：拉链散列表、缓存条目以及地址解码（上 下）](#81-82高速缓存的数据结构拉链散列表缓存条目以及地址解码上-下)
  - [83-84、结合硬件级别的缓存数据结构深入分析缓存一致性协议（上 下）](#83-84结合硬件级别的缓存数据结构深入分析缓存一致性协议上-下)
  - [85、采用写缓冲器和无效队列优化MESI协议的实现性能](#85采用写缓冲器和无效队列优化mesi协议的实现性能)
  - [86、硬件层面的MESI协议为何会引发有序性和可见性的问题？](#86硬件层面的mesi协议为何会引发有序性和可见性的问题)
  - [87、内存屏障在硬件层面的实现原理以及如何解决各种问题](#87内存屏障在硬件层面的实现原理以及如何解决各种问题)
  - [88、在复杂的硬件模型之上的Java内存模型是如何大幅简化的？](#88在复杂的硬件模型之上的java内存模型是如何大幅简化的)
  - [89、面试的时候如何从内存屏障、硬件层面的原理来震慑面试官](#89面试的时候如何从内存屏障硬件层面的原理来震慑面试官)
  - [90-91、Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁（上 下）](#90-91java虚拟机对锁的优化锁消除锁粗化偏向锁自旋锁上-下)
  - [92、再来看看CAS是如何基于MESI协议在底层硬件层面实现加锁的？](#92再来看看cas是如何基于mesi协议在底层硬件层面实现加锁的)

## java 取模与取余的区别

1, 对于整型数a，b来说，取模运算或者求余运算的方法都是：
  
- 第一步：求 整数商： c = a/b;

- 第二步：计算模或者余数： r = a - c * b

> 求模运算和求余运算在第一步不同: 取余运算在取c的值时，向0 方向舍入(fix()函数)；而取模运算在计算c的值时，向负无穷方向舍入(floor()函数)。

例如计算：-7 Mod 4
那么：a = -7；b = 4；

- 第一步：求整数商c，如进行求模运算c = -2（**向负无穷方向舍入**），求余c = -1（**向0方向舍入**）；

- 第二步：计算模和余数的公式相同，但因c的值不同，求模时r = 1，求余时r = -3。

2, 归纳：当a和b符号一致时，求模运算和求余运算所得的c的值一致，因此结果一致。
  
- 当符号不一致时，结果不一样。**求模运算结果的符号和b（除数）一致，求余运算结果的符号和a（被除数）一致**。

- 另外各个环境下%运算符的含义不同，比如c/c++，java 为取余，而python则为取模。

3，Java程序示例：

```
public static void main(String[] args) {
        System.out.println("-3,2取模"+Math.floorMod(-3,2));
        System.out.println("-3,2取余"+ -3%2);
        System.out.println("3,-2取模"+Math.floorMod(3,-2));
        System.out.println("3,-2取余"+ 3%-2);
    }

```

运行结果：

``` 
-3,2取模=1
-3,2取余=-1
3,-2取模=-1
3,-2取余=1

```

## 你知道HashMap底层的数据结构是什么吗?

1，底层的数据结构是：**数组**

详情说明(举例)：

``` 
HashMap<String, String> map = new HashMap<String, String>();
map.put(“张三”, “测试数据”);
map.put(“李四”, “测试数据”);
```

- hashmap的存值过程：
  
对key"张三"计算出来一个hash值，根据这个hash值对数组进行取模，就会定位到数组里的一个元素中去.

``` 
[<>, <>, <>, <>,<张三, 测试数据>, <>,<>,<李四, 测试数据>,<>, <>, <>, <>,<>, <>, <>, <>]

array[4] = <张三, 测试数据>
```

- hashmap取值的过程

map.get(“张三”) -> hash值 -> 对数组长度进行取模 -> return array[4]

## JDK 1.8中对hash算法和寻址算法是如何优化的呢?

1,hash算法的优化：对每个hash值，在他的低16位中，让高低16位进行了异或，让他的低16位同时保持了高低16位的特征，尽量避免一些hash值后续出现冲突，大家可能会进入数组的同一个位置

2,寻址算法的优化：用与运算替代取模，提升性能

- hash算法优化详情：

源码

``` 
      // JDK 1.8以后的HashMap里面的一段源码
    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}

```

``` 
// key.hashCode()比如说：有一个key的hash值 A
1111 1111 1111 1111 1111 1010 0111 1100
// A >>> 16 这是向右移动16位后的值 B
0000 0000 0000 0000 1111 1111 1111 1111 
// 这是 A ^ B  (A 异或 B) 的值
1111 1111 1111 1111 0000 0101 1000 0011 -> int值，32位。
```

这样就实现了：让他的低16位同时保持了高低16位的特征，尽量避免一些hash值后续出现的冲突

>因为hash值一样 -> 他们其实都会在数组里放在一个位置，进行复杂的hash冲突的处理，将影响性能。

hash值对数组长度取模，定位到数组的一个位置，将key-value塞数组里面就ok了

- 寻址算法优化

优化的核心点是：

```
将hash对n取模 -> 数组里的一个位置 

改成：
(n - 1) & hash -> 数组里的一个位置

得到的数组位置是一样的，(n - 1) & hash的性能更高。

同时注意：（n-1）主要是和 hash 的低16位进行与运算。
高16位之间的与运算，是可以忽略的，核心点在于低16位的与运算，hash值的高16位没有参与到与运算里来。

// 当只有高16位不一样时，我们进行高16位低16位异或运算，从而实现hash的低16位不一样。
1111 1111 1111 1111 1111 1010 0111 1100 -> （异或后：）1111 1111 1111 1111 0000 0101 1000 0011

1111 1111 1111 1110 1111 1010 0111 1100 -> （异或后：）1111 1111 1111 1110 0000 0101 1000 0010

```

hash & (n - 1) -> 效果是跟hash对n取模，效果是一样的，但是与运算的性能要比hash对n取模要高很多，数学问题，**数组的长度会一直是2的n次方**。

>说明: hash & (n - 1) 中的hash值也是经过上面优化过后的值（即：他的低16位同时保持了高低16位的特征）

## 你知道HashMap是如何解决hash碰撞问题的吗？

1，hash冲突问题，链表+红黑树，O(n)和O(logn)。

> 尽管jdk1.8做了 map.put和map.get -> hash算法优化（避免hash冲突），寻址性能优化。

> 算出key的hash值，到数组中寻址，找到一个位置，把key-value对放进数组，或者从数组里取出来

> 但是两个key，多个key，他们算出来的hash的值，和n-1，与运算之后，发现定位出来的数组的位置还是一样的，还是会出现hash碰撞或者叫hash冲突。

- 这时怎么处理呢？
  
1，会在这个位置挂一个链表，这个链表里面放入多个元素，让多个key-value对，同时放在数组的一个位置里。例如：

```
[<> -> <> -> <>, ]
array[0]这个位置，就是一个链表
```

get取值时，如果定位到数组里发现这个位置挂了一个链表，此时遍历链表O(n)，从里面找到自己的要找的那个key-value对就可以了

2，假设你的链表很长，可能会导致遍历链表性能会比较差。

所以链表的长度达到了一定的长度之后，其实会把**链表转换为红黑树**，遍历一颗红黑树找一个元素，此时O(logn)，性能会比链表遍历O(n)高一些。

## 说说HashMap是如何进行扩容的可以吗？

1，2倍扩容
HashMap底层是一个数组，当这个数组满了之后，他就会自动进行扩容，变成一个更大的数组，让你在里面可以去放更多的元素。

2，判断新数组（n-1）& hash 的二进制结果中是否多出一个bit的1，如果没多，那么就是原来的index位置，如果多了出来，那么就是 **原index + oldCap（为原数组length）**，通过这个方式，就避免了rehash的时候，用每个hash对新数组.length取模，取模性能不高，位运算的性能比较高。

3，扩容可能带来的变化过程
[16位的数组，<> -> <> -> <>]
[32位的数组，<> -> <>, <>]
扩容后，原来数组同一位置上的链结构可能被拆分到另外的位置上去。

例如：数组长度是 16时, hash1和hash2会在数组的同一个位置上，出现一个hash冲突的问题，用链表来处理。

``` 
n - 1     0000 0000 0000 0000 0000 0000 0000 1111
hash1     1111 1111 1111 1111 0000 1111 0000 0101
&结果     0000 0000 0000 0000 0000 0000 0000 0101    = 5（index = 5的位置）
 
n - 1    0000 0000 0000 0000 0000 0000 0000 1111
hash2    1111 1111 1111 1111 0000 1111 0001 0101
&结果    0000 0000 0000 0000 0000 0000 0000 0101 = 5（index = 5的位置）

```

如果数组的长度扩容之后 = 32，重新对每个hash值进行寻址，也就是用每个hash值跟新数组的length - 1进行与操作：

```
n-1       0000 0000 0000 0000 0000 0000 0001 1111
hash1     1111 1111 1111 1111 0000 1111 0000 0101
&结果     0000 0000 0000 0000 0000 0000 0000 0101 = 5（index = 5的位置）
 
n-1       0000 0000 0000 0000 0000 0000 0001 1111
hash2     1111 1111 1111 1111 0000 1111 0001 0101
&结果     0000 0000 0000 0000 0000 0000 0001 0101 = 21（index = 21的位置）

```

hash1和hash2将在新数组中的不同位置。位置变化规律请参考第2点。

## 说说synchronized关键字的底层原理是什么？

1,synchronized底层的原理，是跟jvm指令和monitor有关系的。
如果我要是对synchronized往深了讲，他是可以很深很深的，内存屏障的一些东西，cpu之类的硬件级别的原理，原子性、可见性、有序性，指令重排，JDK对他实现了一些优化，偏向锁。

<div align='center'>
<img src=./images/java-基础01/java-基础01_2020-01-06-16-48-48.png width='80%'/></div>
<br/>

你如果用到了synchronized关键字，在底层编译后的jvm指令中，会有monitorenter和monitorexit两个指令

```
monitorenter
 
// 代码对应的指令
 
monitorexit
```

2,那么monitorenter和monitorexit指令执行的时候会干什么呢？

- 他里面的原理和思路大概是这样的，monitor里面有一个计数器，从0开始的。如果一个线程要获取monitor的锁，就看看他的计数器是不是0，如果是0的话，那么说明没人获取锁，他就可以获取锁了，然后对计数器加1
  
- 这个monitor的锁是支持重入加锁的，什么意思呢，好比下面的代码片段。
如果一个线程第一次synchronized那里，获取到了myObject对象的monitor的锁，计数器加1，然后第二次synchronized那里，会再次获取myObject对象的monitor的锁，这个就是重入加锁了，然后计数器会再次加1，变成2

```
// 线程1
synchronized(myObject) {  // 类的class对象来走的,加锁，一般来说都是必须对一个对象进行加锁
// 一大堆的代码
    synchronized(myObject) {
    // 一大堆的代码
    }
}

```

- 每个对象都有一个关联的monitor，比如一个对象实例就有一个monitor，一个类的Class对象也有一个monitor，如果要对这个对象加锁，那么必须获取这个对象关联的monitor的lock锁

- 这个时候，其他的线程在第一次synchronized那里，执行monitorenter会发现说myObject对象的monitor锁的计数器是大于0的，意味着被别人加锁了，然后此时线程就会进入block阻塞状态，什么都干不了，就是等着获取锁
  
- 接着如果线程1出了synchronized修饰的代码片段的范围，在底层，就会有一个monitorexit的指令。此时获取锁的线程就会对那个对象的monitor的计数器减1，如果有多次重入加锁就会对应多次减1，直到最后，计数器是0

- 然后后面block住阻塞的线程，会再次尝试获取锁，但是只有一个线程可以获取到锁。

## 能聊聊你对CAS的理解以及其底层实现原理可以吗 ? 

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-06-17-19-05.png width='80%'/></div><br/>

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-06-17-19-23.png width='100%'/></div><br/>

CAS:取值，询问，修改

多个线程他们可能要访问同一个数据
 
HashMap map = new HashMap();
 
此时有多个线程要同时读写类似上面的这种内存里的数据，此时必然出现多线程的并发安全问题。
 
我们可能要用到并发包下面的很多技术，例如：synchronized

synchronized(map) {
   // 对map里的数据进行复杂的读写处理
}
 
并发包下面的其他的一些技术，例如CAS
 
一段代码（非CAS实现）：
 
<div align='center'><img src=./images/java-基础01/java-基础01_image.png.png width='80%'/></div><br/>
 
此时，synchronized他的意思就是针对当前执行这个方法的myObject对象进行加锁
 
只有一个线程可以成功的堆myObject加锁，可以对他关联的monitor的计数器去加1，加锁，一旦多个线程并发的去进行synchronized加锁，串行化，效率并不是太高，很多线程，都需要排队去执行。

CAS的全称：compare and set 比较和设置。
CAS去进行安全的累加代码：

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-06-17-21-00.png width='80%'/></div><br/>
 
CAS在底层的硬件级别给你保证一定是原子的，同一时间只有一个线程可以执行CAS，先比较再设置，其他的线程的CAS同时间去执行此时会失败

## ConcurrentHashMap实现线程安全的底层原理到底是什么？

1，JDK 1.8以前，多个数组，分段加锁，一个数组一个锁。

2，JDK 1.8以后，优化细粒度，一个数组，每个元素先进行CAS，如果失败说明有人put过值了，此时synchronized对这个数组元素加锁，链表+红黑树处理。jdk1.8+是对数组每个元素加锁。

- 多个线程要访问同一个数据，synchronized加锁，CAS去进行安全的累加，去实现多线程场景下的安全的更新一个数据的效果，比较多的一个场景下，可能就是多个线程同时读写一个HashMap,如果给HashMap加一个synchronized，效率将会很低，也没这个必要。
  
```
// 多个线程过来，线程1要put的位置是数组[5]，线程2要put的位置是数组[21]，
//这种情况下根本不需要synchronized同步，同步了反而会降低效率，明显不好。
map.put(xxxxx,xxx);
//数组里有很多的元素，除非是对同一个元素执行put操作，此时的多线程是需要进行同步的
```

- 所以 JDK并发包里推出了一个ConcurrentHashMap，他默认实现了线程安全性。实现过程如下：

> HashMap的一个底层的原理，本身是一个大的一个数组，[有很多的元素]

（1），在JDK 1.7以及之前的版本里，将这个大的数组分段成多个数组
 
[数组1] , [数组2]，[数组3] -> 每个数组都对应一个锁，分段加锁
 
// 多个线程过来，线程1要put的位置是数组1[5]，线程2要put的位置是数组2[21]

（2），JDK 1.8以及之后，做了一些优化和改进，锁粒度的细化。
 
[一个大的数组]，数组里每个元素进行put操作，都是有一个不同的锁，刚开始进行put的时候，如果两个线程都是在数组[5]这个位置进行put，这个时候，对数组[5]这个位置进行put的时候，采取的是CAS的策略
 
同一个时间，只有一个线程能成功执行这个CAS，就是说他刚开始先获取一下数组[5]这个位置的值，null，线程1然后执行CAS，put进去我的这条数据，同时，其他的线程执行CAS，都会失败
 
通过对数组每个元素执行CAS的策略，如果是很多线程对数组里不同的元素执行put，大家是没有关系的，如果其他人失败了，其他人此时会发现说，数组[5]这位置，已经有人放进去值了，synchronized对数组这个元素加锁。

## 你对JDK中的AQS理解吗？AQS的实现原理是什么？

1，AQS ：Abstract Queue Synchronizer，抽象队列同步器。

```
ReentrantLock lock = new ReentrantLock(true);  => 公平锁
//ReentrantLock lock = new ReentrantLock();  => 非公平锁，默认是非公平锁
// 多个线程过来，都尝试
lock.lock();
// 进来的线程 执行的一堆代码TODO
lock.unlock();

```

上面代码ReentrantLock对象使用时的执行过程
state变量 -> CAS -> 失败后进入队列等待 -> 释放锁后唤醒

2， 实现原理图

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-16-08-08.png width='80%'/></div><br/>

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-16-21-22.png width='80%'/></div><br/>

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-16-28-09.png width='80%'/></div><br/>

> 公平锁执行原理图中： 线程3 先看一下等待队列是否 有人排队，有，直接进入等待队列排队。没有，当然是去获取锁咯。

## 说说线程池的底层工作原理可以吗？

原理图1：

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-17-37-28.png width='80%'/></div><br/>

原理图2：//线程释放后，线程会去队列中争抢获取任务执行的

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-17-40-24.png width='80%'/></div><br/>

代码：

```
ExecutorService threadPool = Executors.newFixedThreadPool(3) // 3: corePoolSize
 
threadPool.submit(new Callable() {
       public void run() {}
})；
```

提交任务，先看一下线程池里的线程数量是否小于corePoolSize，也就是3，如果小于，直接创建一个线程出来执行你的任务
 
如果执行完你的任务之后，这个线程是不会死掉的，他会尝试从一个无界的LinkedBlockingQueue里获取新的任务，如果没有新的任务，此时就会阻塞住，等待新的任务到来
 
你持续提交任务，上述流程反复执行，只要线程池的线程数量小于corePoolSize，都会直接创建新线程来执行这个任务，执行完了就尝试从无界队列里获取任务，直到线程池里有corePoolSize个线程
 
接着再次提交任务，会发现线程数量已经跟corePoolSize一样大了，此时就直接把任务放入队列中就可以了，**线程会争抢获取任务执行的**，如果所有的线程此时都在执行任务，那么无界队列里的任务就可能会越来越多
 
fixed，队列，LinkedBlockingQueue，无界阻塞队列

## 说说线程池的核心配置参数都是干什么的？平时我们应该怎么用？

1，先看一下fixed线程 Executors.newFixedThreadPool(3)的内部代码实现。一般比较常用的也是：fixed线程(任务很多很多会内存溢出)，
其中参数new LinkedBlockingQueue\<Runnable>() 就是设置一个无界阻塞队列的参数。

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-21-59-13.png width='80%'/></div><br/>

2，从上面代码可以看出，代表线程池的类是ThreadPoolExecutor。
而fixed之类的线程池也是在其基础上再封装一层，所以我们完全可以通过这个构造函数就创建自己的线程池配合其参数corePoolSize，maximumPoolSize，keepAliveTime，queue

3，我们用线程池的类ThreadPoolExecutor自定义一个线程池，来说明一下其各个参数的含义：

```

new ThreadPoolExecutor(3,Integer.MAX_VALUE0,60s,new ArrayBlockingQueue<Runnable>(200))

corePoolSize：3  // 线程池 大小为 3
maximumPoolSize：Integer.MAX_VALUE //最多可以创建的线程数，此处暂时设置为 无限个。这样设置，任务太多太多时，会内存溢出或者cpu负载过大而宕机
keepAliveTime：60s  // 当任务全部执行完后，线程池中超过corePoolSize的线程存活时间为60s后，再自动销毁。
new ArrayBlockingQueue<Runnable>(200) // 线程中的队列可以存放200个任务

```

- 如果说你把queue做成有界队列，比如说new ArrayBlockingQueue\<Runnable>(200)，那么假设corePoolSize个线程都在繁忙的工作，大量任务进入有界队列，队列满了，此时怎么办？

> 这个时候假设你的maximumPoolSize是比corePoolSize大的，此时会继续创建额外的线程放入线程池里，来处理这些任务，然后超过corePoolSize数量的线程如果处理完了一个任务也会尝试从队列里去获取任务来执行

- 如果maximumPoolSize参数设置为 300 时 额外线程都创建完了去处理任务，队列还是满的，此时还有新的任务来怎么办？

> 只能reject掉，他有几种reject策略，可以传入RejectedExecutionHandler
(1)AbortPolicy   // 抛异常 将抛出 RejectedExecutionException
(2)DiscardPolicy   // 抛弃新任务
(3)DiscardOldestPolicy  // 抛弃旧任务
(4)CallerRunsPolicy  
(5)自定义  // 例如将任务写入磁盘，待线程空闲时再处理

根据上述原理去定制自己的线程池，考虑到corePoolSize的数量，队列类型，最大线程数量，拒绝策略，线程释放时间

4，最后我没看一下执行原理图：

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-07-23-07-06.png width='80%'/></div><br/>

- 面试题： 在远程服务异常的情况下，使用无界阻塞队列，是否会导致内存异常飙升？（如果在线程池中使用无界阻塞队列会发生什么问题？）
  
> 调用超时，队列变得越来越大，此时会导致内存飙升起来，而且还可能会导致你会OOM，内存溢出。

- 你知道如果线程池的队列满了之后，会发生什么事情吗？

> 由题可知，此线程池是个有界队列。
>
> 第一种情况，如果maximumPoolSize设置的是（Integer.MAX_VALUE ）无限大，则可能会出现：
> 由于可以无限制的不停的创建额外的线程出来，一台机器上，有几千个线程，甚至是几万个线程，每个线程都有自己的栈内存，占用一定的内存资源，会导致内存资源耗尽，系统也会崩溃掉，就算内存没有崩溃，也会导致你的机器的cpu load，负载，特别的高。
> 
> 第二种情况，如果maximumPoolSize设置了一个上限值。很有可能会抛异常和丢掉很多未执行的任务。
> 
> 解决第二种情况的方案可以参考这样的：
> 自定义一个reject策略，如果线程池无法执行更多的任务了，此时建议你可以把这个任务信息持久化写入磁盘里去，后台专门启动一个线程，后续等待你的线程池的工作负载降低了，他可以慢慢的从磁盘里读取之前持久化的任务，重新提交到线程池里去执行。

## 如果线上机器突然宕机，线程池的阻塞队列中的请求怎么办？

1，必然会导致线程池里的积压的任务实际上来说都是会丢失的

2，针对上面的问题我们如何解决呢？

> 如果说你要提交一个任务到线程池里去，在提交之前，麻烦你先在数据库里插入这个任务的信息，更新他的状态：未提交、已提交、已完成。提交成功之后，更新他的状态是已提交状态
系统重启，后台线程去扫描数据库里的未提交和已提交状态的任务，可以把任务的信息读取出来，重新提交到线程池里去，继续进行执行

## 谈谈你对Java内存模型的理解可以吗？

1，用下面的 2个线程来说明java内存模型的过程

代码图:
<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-08-15-37-07.png width='80%'/></div><br/>

执行过程图(默认为未加锁的执行图):

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-08-15-43-26.png width='100%'/></div><br/>

2, 内存间的交互操作 read、load、use、assign、store、write 这6个指令的含义

- lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。
- unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。
- read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 <font color="red"> *工作内存（cpu级别的缓存空间）* </font>的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。
- use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。
- assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。
- store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。
- write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。

## 你知道Java内存模型中的原子性、有序性、可见性是什么吗？

1,也就是并发编程过程中，可能会产生的三类问题
默认是没有可见性，没有原子性，没有有序性

2，什么是可见性，原子性，有序性？并发编程时又如何去保证这三性呢？

- 可见性：可见性是一种复杂的属性，因为可见性中的错误总是会违背我们的直觉。通常，我们无法确保执行读操作的线程能适时地看到其他线程写入的值，有时甚至是根本不可能的事情。为了确保多个线程之间对内存写入操作的可见性，必须使用同步机制。
<font color="red">可见性： 是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。</font>也就是一个线程修改的结果。另一个线程马上就能看到。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，即直接修改内存。所以对其他线程是可见的。但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是一个非原子操作，也就是这个操作同样存在线程安全问题。

>在 Java 中 volatile、synchronized 和 final 实现可见性。

- 原子性：原子是世界上的最小单位，**具有不可分割性**。比如 a=0；（a非long和double类型） 这个操作是不可分割的，那么我们说这个操作时原子操作。再比如：a++； 这个操作实际是a = a + 1；是可分割的，所以他不是一个原子操作。非原子操作都会存在线程安全问题，需要我们使用同步技术（sychronized）来让它变成一个原子操作。一个操作是原子操作，那么我们称它具有原子性。java的concurrent包下提供了一些原子类，我们可以通过阅读API来了解这些原子类的用法。比如：AtomicInteger、AtomicLong、AtomicReference等。

>在 Java 中 synchronized 和在 lock、unlock 中操作保证原子性。

- 有序性：线程内的所有操作都是有序的，既程序执行的顺序按照代码的先后顺序执行。

> 重排序是对内存访问操作的一种优化，他可以在不影响单线程程序正确性的前提下进行一定的调整，进而提高程序的性能，但是对于多线程场景下，就可能产生一定的问题

多线程场景下的代码，可能会出现指令重排序编译器和指令器，有的时候为了提高代码执行效率，会将指令重排序，就是说比如下面的代码。

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-08-17-13-39.png width='80%'/></div><br/>

如果重排序之后，让flag = true先执行了，会导致线程2直接跳过while等待，执行某段代码，结果prepare()方法还没执行，资源还没准备好呢，此时就会导致代码逻辑出现异常。

>Java 语言提供了 volatile 和 synchronized 两个关键字来保证线程之间操作的有序性，volatile 是因为其本身包含“禁止指令重排序”的语义，synchronized 是由“一个变量在同一个时刻只允许一条线程对其进行 lock 操作”这条规则获得的，此规则决定了持有同一个对象锁的两个同步块只能串行执行。

## 能从Java底层角度聊聊volatile关键字的原理吗？

1,volatile关键字是用来解决可见性和有序性,可以有限的保证原子性。
当volatile修饰的变量被某一个线程修改后，则其他线程中的这个变量原来的值将会**失效**，从而迫使这些线程从新去主内存中读取加载这个变量的新值。从而保证了可见性。

2，下面举例说明：

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-09-53-11.png width='80%'/></div><br/>

volatile修饰的变量被一个线程修改后，其他线程中原来的值将会失效。

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-09-40-21.png width='80%'/></div><br/>

3，经常应用的场景：volatile修饰的变量 有线程修改其值，有线程要读取其值。
例如：在很多的开源中间件系统的源码里，大量的使用了volatile，每一个开源中间件系统，或者是大数据系统，都多线程并发，volatile

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-09-51-30.png width='80%'/></div><br/>

## 你知道指令重排以及happens-before原则是什么吗？

1，指令重排：编译器、指令器可能对代码重排序，乱排，但要守happens-before原则，只要符合happens-before的原则，那么就不能重排，如果不符合这些规则的话，那就可以自己排序，以提高执行效率。
> 计算机在执行程序过程：
> 源代码 -> 编译器优化的重排 -> 指令并行的重排 -> 内存系统的重排 -> 最终执行的指令

2，happens-before原则：

- **程序次序规则：** 一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作（所以<font color =red>单线程不会出现指令重排</font>）
- **锁定规则：** 一个unLock操作先行发生于后面对同一个锁的lock操作，比如说在代码里有先对一个lock.lock()，lock.unlock()，lock.lock()
- **volatile变量规则：** 对一个volatile变量的写操作先行发生于后面对这个volatile变量的读操作，volatile变量写，再是读，必须保证是先写，再读
- **传递规则：** 如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C
- **线程启动规则：** Thread对象的start()方法先行发生于此线程的每个一个动作，thread.start()，thread.interrupt()
- **线程中断规则：** 对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生
- **线程终结规则：** 线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行
- **对象终结规则：** 一个对象的初始化完成先行发生于他的finalize()方法的开始

> 上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序，即不允许编译器、指令器对你写的代码进行指令重排，必须保证你的代码的有序性。但是如果没满足上面的规则，那么就可能会出现指令重排，就这个意思。

3，volatile三个特性，保证可见性，不保证原子性，禁止指令重排。

- 比如这个例子，如果用volatile来修饰flag变量，一定可以让prepare()指令在flag = true之前先执行，这就禁止了指令重排。

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-10-44-08.png width='60%'/></div><br/>

>因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作的后面，volatile后面的代码也不能指令重排到volatile变量操作的前面。

## volatile底层是如何基于内存屏障保证可见性和有序性的？

1，volatile保证可见性和有序性，不保证原子性。

2，volatile底层原理，如何实现保证可见性的呢？

> 通过 lock前缀指令 + MESI缓存一致性协议
> - lock指令：volatile保证可见性 
对volatile修饰的变量，执行写操作的话，JVM会发送一条lock前缀指令给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行嗅探，自己本地缓存中的数据是否被别人修改
> - 如果发现别人修改了某个缓存的数据，那么CPU就会将自己本地缓存的数据过期掉(失效)，然后这个CPU上执行的线程在读取那个变量的时候，就会从主内存重新加载最新的数据了

3, volatile底层原理，如何实现保证有序性的呢？

> 通过插入内存屏障又叫内存栅栏(是一个CPU指令)，就能禁止在内存屏障前后的指令执行重排优化。内存屏障另外一个作用就是强制刷出各种CPU的缓存数据。
> 内存屏障有：LoadLoad屏障, StoreStore屏障, LoadStore屏障, StoreLoad屏障等等。

## 说说你对Spring的IOC机制和AOP机制的理解可以吗？

1,Spring IOC框架，控制反转，依赖注入。

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-15-45-54.png width='100%'/></div><br/>

- spring ioc的实现过程

> 我们只要在这个工程里通过maven引入一些spring框架的依赖，ioc功能
> tomcat在启动的时候，直接会启动spring容器
> spring ioc，spring容器，根据xml配置，或者是你的注解，去实例化你的一些bean对象，然后根据xml配置或者注解，去对bean对象之间的引用关系，去进行依赖注入，某个bean依赖了另外一个bean。

- spring ioc底层实现的核心技术是
  
> 反射，他会通过反射的技术，直接根据你的类去自己构建对应的对象出来，用的就是反射技术

- spring ioc 主要解决了什么问题

> 系统的类与类之间彻底的解耦合

2, 从没有 IOC 的代码实现 和 有 IOC 代码实现的 对比来看ioc的意义。

- 无ioc时的代码
写一套系统，web服务器，tomcat，一旦启动之后，他就可以监听一个端口号的http请求，然后可以把请求转交给你的servlet，jsp，配合起来使用的，servlet处理请求

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-16-04-22.png width='80%'/></div><br/>

> 在我们的一个tomcat+servlet的这样的一个很low的系统里，有几十个地方，都是直接用MyService myService = new MyServiceImpl()，直接创建、引用和依赖了一个MyServiceImpl这样的一个类的对象。这就有几十个地方，都跟MyServiceImpl类直接耦合在一起了。

> 我现在不想要用MyServiceImpl了，我们希望用的是NewServiceManagerImpl，implements MyService这个接口的，所有的实现逻辑都不同了，此时我们很麻烦，我们需要在很low的系统里，几十个地方，都去修改对应的MyServiceImpl这个类，切换为NewServiceManagerImpl这个类

> 这样写的代码的缺点是 ：
改动代码成本很大，改动完以后的测试的成本很大，改动的过程中可能很复杂，出现一些bug，此时就会很痛苦，归根结底，代码里，各种类之间完全耦合在一起，出现任何一丁点的变动，都需要改动大量的代码，重新测试，可能还会有bug

- 有ioc 的代码，xml文件来进行一个配置，进化到了基于注解来进行自动依赖注入。（实现类与类之间的解耦）
  
  <div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-16-11-50.png width='80%'/></div><br/>

> 现在这套比较高大上的一点系统里，有几十个类都使用了@Resource这个注解去标注MyService myService，几十个地方都依赖了这个类，如果要修改实现类MyServiceImpl为NewServiceManagerImpl时，只有将 @Service注解改到NewServiceManagerImpl类上即可。

## 说说你对Spring的AOP机制的理解可以吗？

1，AOP（Aspect-OrientedProgramming，面向方面编程），可以说是OOP（Object-Oriented Programing，面向对象编程）的补充和完善。
用于处理系统中分布于各个模块的横切关注点，比如事务管理、日志、缓存等等。AOP实现的关键在于AOP框架自动创建的AOP代理，AOP代理主要分为静态代理和动态代理，静态代理的代表为AspectJ；而动态代理则以Spring AOP为代表。静态代理是编译期实现，动态代理是运行期实现，可想而知前者拥有更好的性能。

2，实现AOP的技术
主要分为两大类：一是采用**动态代理技术**，利用截取消息的方式，对该消息进行装饰，以取代原有对象行为的执行；二是采用**静态织入**的方式，引入特定的语法创建“方面”，从而使得编译器可以在编译期间织入有关“方面”的代码。

3，AOP用来封装横切关注点，具体可以在下面的场景中使用:

Authentication 权限,
Caching 缓存,
Context passing 内容传递,
Error handling 错误处理,
Lazy loading　懒加载,
Debugging　　调试,
logging, tracing, profiling and monitoring　记录跟踪　优化　校准,
Performance optimization　性能优化,
Persistence　　持久化,
Resource pooling　资源池,
Synchronization　同步,
Transactions 事务,

4，AOP 的作用

> AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。

5, AOP中相关的概念定义

- Aspect（切面）： Aspect 声明类似于 Java 中的类声明，在 Aspect 中会包含着一些 Pointcut 以及相应的 Advice。
- Joint point（连接点）：表示在程序中明确定义的点，典型的包括方法调用，对类成员的访问以及异常处理程序块的执行等等，它自身还可以嵌套其它 joint point。
- Pointcut（切点）：表示一组 joint point，这些 joint point 或是通过逻辑关系组合起来，或是通过通配、正则表达式等方式集中起来，它定义了相应的 Advice 将要发生的地方。
- Advice（增强）：Advice 定义了在 Pointcut 里面定义的程序点具体要做的操作，它通过 before、after 和 around 来区别是在每个 joint point 之前、之后还是代替执行的代码。
- Target（目标对象）：织入 Advice 的目标对象.。
- Weaving（织入）：将 Aspect 和其他对象连接起来, 并创建 Adviced object 的过程

这些概念之间关系的图：

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-17-08-00.png width='100%'/></div><br/>

6，Advice 的类型

- before advice, 在 join point 前被执行的 advice. 虽然 before advice 是在 join point 前被执行, 但是它并不能够阻止 join point 的执行, 除非发生了异常(即我们在 before advice 代码中, 不能人为地决定是否继续执行 join point 中的代码)
- after return advice, 在一个 join point 正常返回后执行的 advice
- after throwing advice, 当一个 join point 抛出异常后执行的 advice
- after(final) advice, 无论一个 join point 是正常退出还是发生了异常, 都会被执行的 advice.
- around advice, 在 join point 前和 joint point 退出后都执行的 advice. 这个是最常用的 advice.
- introduction，introduction可以为原有的对象增加新的属性和方法。

7，Spring AOP的两种代理实现机制**JDK动态代理**和**CGLIB动态代理**

- 静态代理是编译阶段生成AOP代理类，也就是说生成的字节码就织入了增强后的AOP对象；动态代理则不会修改字节码，而是在内存中临时生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。

- Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理。JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是InvocationHandler接口和Proxy类。

- 如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意，CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的，诸如private的方法也是不可以作为切面的。

8，Spring AOP中的JDK动态代理的举例。AOP的核心技术，就是动态代理

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-17-30-58.png width='80%'/></div><br/>

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-17-31-43.png width='80%'/></div><br/>

## 了解过cglib动态代理吗？他跟jdk动态代理的区别是什么？

1，spring里使用aop，比如说你对一批类和他们的方法做了一个切面，定义好了要在这些类的方法里增强的代码，spring必然要对那些类生成动态代理，在动态代理中去执行你定义的一些增强代码。

2，其实就是动态的创建一个代理类出来，创建这个代理类的实例对象，在这个里面引用你真正自己写的类，所有的方法的调用，都是先走代理类的对象，他负责做一些代码上的增强，再去调用你写的那个类。优先是jdk动态代理，其次是cglib动态代理

3，如果你的类是实现了某个接口的，spring aop会使用jdk动态代理，生成一个跟你实现同样接口的一个代理类，构造一个实例对象出来，jdk动态代理，他其实是在你的类有接口的时候，就会来使用。

4，很多时候我们可能某个类是没有实现接口的，spring aop会改用cglib来生成动态代理，他是生成你的类的一个子类，他可以动态生成字节码，覆盖你的一些方法，在方法里加入增强的代码

5，代码示例链接

- [JDK动态代理代码示例](https://www.cnblogs.com/muscleape/p/9018302.html)
- [cglib动态代理代码示例](https://www.cnblogs.com/muscleape/p/9018308.html)

## 能说说Spring中的Bean是线程安全的吗？

1,答案是否定的，绝对不可能是线程安全的，spring bean默认来说，singleton，都是线程不安全的，java web系统，一般来说很少在spring bean里放一些实例变量，一般来说他们都是多个组件互相调用，最终去访问数据库的

2,Spring容器中的bean可以分为5个范围：
 
- singleton：默认，每个容器中只有一个bean的实例

- prototype：为每一个bean请求提供一个实例
 
一般来说下面几种作用域，在开发的时候一般都不会用，99.99%的时候都是用singleton单例作用域

- request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收
- session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效
- global-session

图解

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-19-35-50.png width='100%'/></div><br/>

上图解对应的代码：（不能这样写）

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-19-40-38.png width='100%'/></div><br/>

一般我们都是这样写的，最后多线程访问数据库。

<div align='center'><img src=./images/java-基础01/java-基础01_2020-01-09-19-17-46.png width='80%'/></div><br/>

## 30、Spring的事务实现原理是什么？能聊聊你对事务传播机制的理解吗？

1，事务的实现原理

如果说你加了一个@Transactional注解，此时就spring会使用AOP思想，对你的这个方法在执行之前，先去开启事务，执行完毕之后，根据你方法是否报错，来决定回滚还是提交事务

2，事务传播机制

嵌套事务，外层的事务如果回滚，会导致内层的事务也回滚；但是内层的事务如果回滚，仅仅是回滚自己的代码
 
① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。也是默认事物。
 
② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。
 
③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。
 
④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。
 
⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
 
⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。
 
⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-20-16-20-22.png width='80%'/></div><br/>

```
A+① 和 B+①/②/③
// 开启一个事务 
// 执行方法A的代码，接着执行方法B的代码
// 提交或者回滚事务
 
 A+① 和 B+④
// 开启一个事务1
// 执行方法A里的一些代码，doSomethingPre()
// 开启一个事务2
// 执行方法B里的一些代码
// 提交或者回滚事务2
// 执行方法A里的一些代码，doSomethingPost()
// 提交或者回滚事务1
 
 A+① 和 B+⑦
// 开启一个事务
// 执行方法A里的一些代码，doSomethingPre()
// 设置一个回滚点，savepoint
// 执行方法B里的一些代码
// 如果方法B里抛出了异常，此时进行回滚，回滚到之前的savepoint
// 执行方法A里的一些代码，doSomethingPost()
// 提交或者回滚事务，如果A回滚，则会将B一起全部回滚了
```

3 出去面试，事务传播机制
 
比如说，我们现在有一段业务逻辑，方法A调用方法B，我希望的是如果说方法A出错了，此时仅仅回滚方法A，不能回滚方法B，必须得用REQUIRES_NEW，传播机制，因为他们俩的事务是不同的。
 
NESTED嵌套事务
方法A调用方法B，如果出错，方法B只能回滚他自己，方法A可以带着方法B一起回滚。

## 31、能画一张图说说Spring Boot的核心架构吗？

spring的时候，带一下spring boot，有时候出去面试，也会问到spring boot，提的一个点，spring boot启动的时候一个流程图，本身他是spring这个项目发展到一定阶段之后的一个产物
 
spring框架，mybatis，spring mvc，去做一些开发，打包部署到线上的tomcat里去，tomcat启动了，他就会接收http请求，转发给spring mvc框架，调用controller -> service -> dao -> mybatis（sql语句）
 
java web开发的时候，在这里整合进来redis、elasticsearch、还有很多其他的一些东西，rabbitmq、zookeeper，等等，诸如此类的一些东西
 
国外的spring开源社区，就发起了一个项目，spring boot，我们基于spring boot直接进行开发，里面还是使用spring + spring mvc + mybatis一些框架，我们可以一定程度上来简化我们之前的开发流程
 
做很多的配置，自己去定义一些bean，流程比较繁琐
 
spring boot内嵌一个tomcat去直接让我们一下子就可以把写好的java web系统给启动起来，直接运行一个main方法，spring boot就直接把tomcat服务器给跑起来，把我们的代码运行起来了
 
自动装配，比如说我们可以引入mybatis，我其实主要引入一个starter依赖，他会一定程度上个自动完成mybatis的一些配置和定义，不需要我们手工去做大量的配置了，一定程度上简化我们搭建一个工程的成本
 
引入一些mybatis的jar包，还有mybatis依赖的一些其他的jar包，然后动手编写一些xml配置文件，然后定义一些bean，写一些sql语句，写一些dao代码，此时就可以使用mybatis去执行sql语句了
 
只要引入一个starter，他会自动给你引入需要的一些jar包，做非常简单的、必须的一些配置，比如数据库的地址，几乎就不用你做太多的其他额外的配置了，他会自动帮你去进行一些配置，定义和生成对应的bean
 
生成的bean自动注入比如你的dao里去，让你免去一些手工配置+定义bean的一些工作
 
spring boot + spring + spirng mvc + mybatis + XXX之类的技术去进行开发，后续很多配置和定义的一些繁琐的重复性的工作就免去了，自动装配的一些功能，自动给你把一些事情干完了，不需要你去做了
 
spring boot这个框架，面试突击第三季，仅仅只是扫盲，源码流程
 
spring boot关键的一些原理，和架构，画一张图，10来分钟的小视频，对spring boot的来龙去脉，有一个基本的了解和认识，基于spring boot开发的时候，他大致的一个工作流程是什么样子的
 
main，他自动启动一个内嵌的tomcat，然后扫描bean, 并进行自动装配。
springboot 就是为了简化开发，约定大于配置。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-20-17-11-10.png width='100%'/></div><br/>

## 32、额外加餐：能画一张图说说Spring的核心架构吗？

spring核心源码，spring核心架构图，里面包含了各种类和API之间的调用，引入一个别的点，把spring的核心的东西再梳理一下
 
spring bean生命周期，从创建 -> 使用 -> 销毁
 
你在系统里用xml或者注解，定义一大堆的bean
 
（1）实例化Bean：如果要使用一个bean的话
 
（2）设置对象属性（依赖注入）：他需要去看看，你的这个bean依赖了谁，把你依赖的bean也创建出来，给你进行一个注入，比如说通过构造函数，setter

（3）处理Aware接口：
 
如果这个Bean已经实现了ApplicationContextAware接口，spring容器就会调用我们的bean的setApplicationContext(ApplicationContext)方法，传入Spring上下文，把spring容器给传递给这个bean
 
（4）BeanPostProcessor：
 
如果我们想在bean实例构建好了之后，此时在这个时间带你，我们想要对Bean进行一些自定义的处理，那么可以让Bean实现了BeanPostProcessor接口，那将会调用postProcessBeforeInitialization(Object obj, String s)方法。
 
（5）InitializingBean 与 init-method：
 
如果Bean在Spring配置文件中配置了 init-method 属性，则会自动调用其配置的初始化方法。
 
（6）如果这个Bean实现了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法
 
（7）DisposableBean：
 
当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法；
 
（8）destroy-method：
 
最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。

创建+初始化一个bean -> spring容器管理的bean长期存活 -> 销毁bean（两个回调函数）

## 33、能说说Spring中都使用了哪些设计模式吗？

工厂，单例，代理。

工厂模式，单例模式，代理模式

工厂模式，spring ioc核心的设计模式的思想提现，他自己就是一个大的工厂，把所有的bean实例都给放在了spring容器里（大工厂），如果你要使用bean，就找spring容器就可以了，你自己不用创建对象了

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-21-14-28-20.png width='80%'/></div><br/>

单例模式，spring默认来说，对每个bean走的都是一个单例模式，确保说你的一个类在系统运行期间只有一个实例对象，只有一个bean，用到了一个单例模式的思想，保证了每个bean都是单例的
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-21-14-28-57.png width='80%'/></div><br/>

代理模式
稍微还算是有点含金量的
如果说你要对一些类的方法切入一些增强的代码，会创建一些动态代理的对象，让你对那些目标对象的访问，先经过动态代理对象，动态代理对象先做一些增强的代码，再调用你的目标对象

在设计模式里，就是一个代理模式的体现和运用，让动态代理的对象，去代理了你的目标对象，在这个过程中做一些增强的访问。

## 34、额外加餐：能画一张图说说Spring Web MVC的核心架构吗？

（1）tomcat的工作线程将请求转交给spring mvc框架的DispatcherServlet

（2）DispatcherServlet查找@Controller注解的controller，我们一般会给controller加上你@RequestMapping的注解，标注说哪些controller用来处理哪些请求，此时根据请求的uri，去定位到哪个controller来进行处理

（3）根据@RequestMapping去查找，使用这个controller内的哪个方法来进行请求的处理，对每个方法一般也会加@RequestMapping的注解

（4）他会直接调用我们的controller里面的某个方法来进行请求的处理

（5）我们的controller的方法会有一个返回值，以前的时候，一般来说还是走jsp、模板技术，我们会把前端页面放在后端的工程里面，返回一个页面模板的名字，spring mvc的框架使用模板技术，对html页面做一个渲染；返回一个json串，前后端分离，可能前端发送一个请求过来，我们只要返回json数据

（6）再把渲染以后的html页面返回给浏览器去进行显示；前端负责把html页面渲染给浏览器就可以了

## 35、额外加餐：能画一张图说说Spring Cloud的核心架构吗？

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-21-15-43-09.png width='100%'/></div><br/>

spring boot、spring、spring mvc、spring cloud，让你开发那种单体架构的系统，spring cloud是让你去开发分布式系统，让你把系统拆分为很多的子系统，子系统互相之间进行请求和调用

面试突击第二季，有完整的spring cloud架构原理的讲解

eureka、ribbon、feign、zuul、hystrix、链路追踪、其他组件，服务于分布式系统的，hystrix主要用于服务之间调用的熔断、隔离、降级，在狸猫技术窝上，在我的课程的目录里，有一个文档，标识出来了我的一些之前的课程，csdn上去搜索，亿级流量里面带有hystrix讲解，看一下

## 36、JVM中有哪几块内存区域？Java 8之后对内存分代做了什么改进？

集合、并发、spring框架，期待着我对这些基础的东西做一些很深入的，很牛X的讲解，基于框架写一些代码，完事儿了之后，就会把代码进行一个部署，一般来说是通过tomcat、jetty来部署java web系统

tomcat部署，tomcat自己就是基于java来开发的，我们启动的不是自己的系统，是一个tomcat是一个jvm进程，我们写的系统只不过是一些代码，放在tomcat的目录里，tomcat会去加载我们的代码到jvm里去

tomcat去负责接收请求，执行我们写好的代码，基于spring框架的一大堆代码

跟面试常问的一些思路结合起来，让大家可以站在面试的角度去思考一下，jvm平时面试会怎么来问呢，如何为了面试去好好准备jvm的东西呢

执行我们的一些对象的方法，执行代码的时候肯定会有很多的线程，tomcat里就有很多自己的工作线程，去执行我们写的代码，每个工作线程都会有自己的一块数据结构(栈内存,每个线程都独有一份)，这个里面是存放一些东西

java 8以后的内存分代的改进，
java8之前: 永久代里放了一些常量池+类信息，
java8:  常量池 -> 堆里面，类信息 -> metaspace（元区域）

## 37、你知道JVM是如何运行起来的吗？我们的对象是如何分配的？

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-21-16-23-39.png width='100%'/></div><br/>

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-21-16-24-44.png width='80%'/></div><br/>

一定会有线程去执行我们写的代码

比如说我们有一个类里面包含了一个main方法，你去执行这个main方法，此时会自动一个jvm进程，他会默认就会有一个main线程，这个main线程就负责执行这个main方法的代码，进而创建各种对象

tomcat，类都会加载到jvm里去，spring容器而言都会对我们的类进行实例化成bean，有工作线程会来执行我们的bean实例对象里的方法和代码，进而也会创建其他的各种对象，实现业务逻辑

## 38、说说JVM在哪些情况下会触发垃圾回收可以吗？

我们的jvm的内存其实是有限制的，不可能是无限的，昂贵的资源，2核4G的机器，堆内存也就2GB左右，4核8G的机器，堆内存可能也就4G左右，栈内存也需要空间，metaspace区域放类信息也需要空间

在jvm里必然是有一个内存分代模型，年轻代和老年代

比如说给年轻代一共是2GB内存，给老年代是2GB内存，默认情况下eden和2个s的比例：8:1:1，eden是1.6GB，S是0.2GB

如果说eden区域满了，此时必然触发垃圾回收，young gc，ygc，谁是可以回收的垃圾对象呢？就是没有人引用的对象就是垃圾对象

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-09-13-22.png width='80%'/></div><br/>

## 39、说说JVM的年轻代垃圾回收算法？对象什么时候转移到老年代？

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-09-26-31.png width='80%'/></div><br/>

如果说你让代码一边运行，一边有变动，一边判断哪些对象是可以回收的，这个是不现实的，垃圾回收的时候有一个概念，叫做stop the world，停止你的jvm里的工作线程的运行，然后扫描所有的对象，判断哪些可以回收，哪些不可以回收的

年轻代，大部分情况下，对象生存周期是很短的，可能在0.01ms之内，线程执行了3个方法，创建了几个对象，0.01ms之后就方法都执行结束了，此时那几个对象就会在0.01ms之内变成垃圾，可以回收的

100个对象，可能90个对象都是垃圾对象，10个对象是存活的对象，5个

复制算法，一次young gc，年轻代的垃圾回收

三种场景，第一种场景，有的对象在年轻代里熬过了很多次垃圾回收，15次垃圾回收，此时会认为这个对象是要长期存活的对象

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-09-28-50.png width='100%'/></div><br/>

## 40、说说老年代的垃圾回收算法？常用的垃圾回收器都有什么？

老年代对象越来越多，是不是会发现说，老年代的内存空间也会满的，可以不可以使用类似年轻代的复制算法，不合适的，因为老年代里的对象，很多都是被长期引用的，spring容器管理的各种bean

长期存活的对象是比较多的，可能甚至有几百MB

对老年代而言，他里面垃圾对象可能是没有那么多的，标记-清理，找出来那些垃圾对象，然后直接把垃圾对象在老年代里清理掉，标记-整理，把老年代里的存活对象标记出来，移动到一起，存活对象压缩到一片内存空间里去

剩余的空间都是垃圾对象整个给清理掉，剩余的都是连续的可用的内存空间，解决了内存碎片的一个问题

parnew+cms的组合，g1直接分代回收，新版本，慢慢的就是主推g1垃圾回收器了，以后会淘汰掉parnew+cms的组合，jdk 8~jdk 9比较居多一些，parnew+cms的组合比较多一些，是这么一个情况

分成好几个阶段，初始标记，并发标记，并发清理，等等，老年代垃圾回收是比较慢的，一般起码比年轻代垃圾回收慢个10倍以上，cms的垃圾回收算法，刚开始用标记-清理，标记出来垃圾对象，清理掉一些垃圾对象，整理，把一些存活的对象压缩到一起，避免内存碎片的产生

执行一个比较慢的垃圾回收，还要stop the world，需要100mb，此时就会让系统停顿100ms，不能处理任何请求，尽可能的让垃圾回收和工作线程的运行，并发着来执行

## 41、你们生产环境中的Tomat是如何设置JVM参数的？如何检查JVM运行情况？

面试的时候，面试官很多时候都是针对jvm的一些运行原理去深扣，结合我讲的东西，然后去把jvm专栏里面的内容仔细看一下，应付面试都是很容易的，一般来说都会这么问，你们线上系统的生产环境的jvm参数是怎么来配置的，为什么要这么配置，在你们配置的这个参数之下，线上系统jvm运行的情况如何

你确实必须得去看一下你当前生产系统的jvm参数都是如何设置的，如果说你是tomcat部署的java web系统，jvm进程对应的tomcat自己，你的系统仅仅是在tomcat的jvm进程来执行

tomcat的一个配置脚本，catalina脚本里去找一下，jvm专栏都有说明的，里面是有对应的tomcat启动的一些jvm参数的设置

比如通过java命令直接启动你的一个main方法跑起来的系统，就是你自己启动的时候，java命令可以带上一些jvm参数

对你自己系统的jvm参数有一个了解，内存区域大小的分配，每个线程的栈大小，metaspace大小，堆内存的大小，年轻代和老年代分别的大小，eden和survivor区域的大小分别是多少，如果没有设置，会有一些默认值

jvm专栏里，在中间有一些地方，他是讲了一些命令的，可以查看jvm的启动默认参数

垃圾回收器，年轻代是用了什么，老年代，每种垃圾回收器是否有对应的一些特殊的参数有设置，那些特殊的参数分别都是用来干什么的

为什么要这么设置呢？当前线上系统运行的时候，jvm的表现如何？

救火队队长的《从0开始带你成为jvm实战高手》，有大量的实战案例的讲解，业务背景引出，在一定的业务背景之下，如何去进行系统运行时的对象数量的预估，对内存的压力进行预估，对整个jvm运行的状况进行预估

预估完毕之后，根据预估的情况，可以去设置一些jvm参数

进行压测，在压测的时候，其实就需要去观察jvm运行的情况，jstat工具去分析jvm运行的情况，他的年轻代里的eden区域的对象增长的情况，ygc的频率，每次ygc过后有多少对象存活，s能否放的下，老年代对象增长速率，老年代多久会触发一次fgc

就可以根据压测的情况去进行一定的jvm参数的调优，一个系统的QPS，一个是系统的接口的性能，压测到一定程度的时候 ，机器的cpu、内存、io、磁盘的一些负载情况，jvm的表现

可能需要对一些代码进行优化，比如优化性能，或者减轻一点cpu负担，减轻io和磁盘负担，发现jvm的gc过于频繁，内存泄漏，此时就需要对jvm的各个内存区域的大小以及一些参数进行调优

跑到线上实际生产环境里去，运行的过程中，也需要基于一些监控工具，或者是jstat，除了观察系统的QPS和性能，接口可用性，调用成功率，机器的负载，jvm的表现，gc的频率，gc耗时，内存的消耗。

## 42、你在实际项目中是否做过JVM GC优化，怎么做的？

如何通过预估 + 压测，做一份生产环境的jvm参数出来的，如何去观察jvm运行的情况，jvm出现频繁full gc的问题，你有没有尝试过生产环境的系统去进行gc的一个优化，对于这个问题

狸猫技术窝公众号上的救火队队长的《从0开始带你成为jvm实战高手》，有非常详细的案例的分析，通过很多个案例，去分析如何在各种各样奇葩的背景之下，发现jvm的gc很频繁，导致系统卡顿问题

如何一步一步去分析系统的jvm的性能问题，如何去进行jvm gc调优

假设你没看过jvm专栏，自己做过jvm gc的生产调优，恭喜你了，直接实话实说，你当时怎么调优，你们的问题如何暴露出来的，你如何一步一步定位问题的，如何进行调优，最后的结果是什么

你看了jvm专栏，在过程中，或者看完以后，在自己生产环境中根据专栏学习到的知识，去调优过jvm，这个时候，你可以专栏里学习到的知识，去讲。最好对自己系统的生产环境的jvm，进行一个分析，gc频繁的问题

你尽可能的去调优一下参数，如果效果比较好

发现分析了一下生产环境的jvm的运行情况，非常好，并发量很低，几十分钟才一次young gc，存活的对象特别少，几乎都在s区域，老年代几乎没什么对象，几天或者几周才发生一次full gc

在自己本地单机部署，测试环境里，去压测，每秒单机有500并发请求，去观察jvm的运行情况，这个时候他会不会存在频繁gc的问题，你就去调优一下，你就可以基于这个压测的例子去说了

一定要结合你自己的业务，系统，接口，干什么，并发请求，jvm运行的情况，问题出在哪儿，如何调优，效果如何。

## 43、你知道发生OOM之后，应该如何排查和处理线上系统的OOM问题？

狸猫技术窝公众号里的救火队队长的《从0开始带你成为jvm实战高手》

oom可能发生在哪几个区域，解决的一个思路，在jvm里可以设置几个参数，如果一旦jvm发生了oom之后，就会导出一份内存快照，就会有当时的线上内存里的对象的一个情况，可以用MAT这样的工具，可以去分析

无非就是找出来当时的时候占用内存最大的对象都是谁，找出来那些对象是在代码中哪些地方创建出来的，一般来说就是可能会对内存去做一个调优

还是得去参考jvm专栏里的大量的案例背景，从业务背景出发，一步步去说明，在什么样的业务背景之下，为什么会产生oom的问题呢？必然会导致系统可能就是崩溃了，客服会反馈说，XX功能不能用了，说某个系统崩溃了

找他自动导出的内存快照，分析，XX对象，直接去定位代码，修改代码

你一定要把案例的业务、背景和思想给吸收了，就得融入到自己的业务里去，我负责的业务系统，在什么样的情况下，可能说会出现一大批的对象卡在内存里，无法回收，导致我系统没法放更多的对象了

产生OOM，内存泄漏的问题，少数场景在互联网公司，超高并发下的oom问题，瞬时大量存活对象占据内存， 导致没法创建更多的对象了

你也得去思考，甚至去模拟一下，最好可以模拟出来，oom不是你自己的代码，可能是你依赖的第三方的组件，netty导致的，结合自己的项目去一步一步的分析，oom问题的产生，和解决的过程

## 44-46_ 你能聊聊TCP/IP四层网络模型吗？OSI七层网络模型也说一下！(上 中 下)
1，**面试官心理分析**

为啥要问这个？
 
坦白讲，一些大的公司，计算机基础必面，尤其是针对薪资30k以内的工程师，因为薪资30k以内，你还是要干活儿的吧，还没上升到就设计架构就可以的程度吧，你还没到那个高度吧。
 
所以只要你干活儿，你就不可避免要跟机器、网络、cpu、磁盘、内存，成天打交道。而线上系统，计算基础的一些东西，网络、cpu、磁盘、内存，都是关联很大的，比如说你线上系统会不会因为网络故障导致一些问题？cpu负载达到100%了咋办？磁盘读写很慢快满了咋办？内存使用率过高咋办？
 
你起码得有一套自己的计算机功底去支撑你玩儿线上系统吧。所以很多人呢，都说计算机基础没啥用，那这个话呢，也对，也不对。对就在于，你如果毕业出来干简单的crud，这些东西你确实不需要；不对就在于，你如果当个高工，带几个小弟干高并发有压力的线上系统，机器负载很高，很容易出问题，结果你连机器都不敢摸，或者也不知道怎么摸，那不是尴尬了么。
 
所以说，计算机基础，网络、磁盘、cpu、内存，还是得会一点儿基础的
 
作为一个大公司的面试官，一定会考察你这些东西

2，**正餐开始——面试题剖析**

首先要说一下，四层模型和七层模型，我们往往是可以一块儿来聊的。
 
（1）首先我问要明白，为啥要有协议
 
设想一下，各个电脑厂商，比如IBM、苹果啥的，都弄自己的协议，结果就苹果电脑和苹果电脑自己可以通信，和IBM电脑就不可以通信，这不是尴尬么。所以搞一个国际通行的协议，大家都按照这个来，所有电脑都可以通信，不是很好么。
 
此时就必须搞一个标准的网络模型出来，大家都按照这个来走，大家都要遵守统一的规范。这就是所谓OSI七层模型，他们分别是：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。那么在这个基础上，又简化出了TCP/IP四层模型，数据链路层、网络层、传输层、应用层。

那么每一层代表的是啥，我一一给大家讲解

（2）从底向上的网络分层
 
1）物理层
 
物理层，物理层干啥的，就是电脑之间要联网，一般咋弄？类似于说，你有台电脑，现在要联网，咋联？以前N年前，大家记不记得都是在电脑上插根线是吧，然后才能上网，结果现在就是联个wifi就行了，还有中国美国之前联网靠的是海底的光缆。所以物理层就指的这个，就是怎么把各个电脑给联结起来，形成一个网络，这就是物理层的含义，物理层负责传输0和1的电路信号。学过一些计算机的同学，计算机的最最底层，就是0/1，电信号。如下图：
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-10-48-03.png width='100%'/></div><br/>

2）数据链路层（以太网协议+Mac地址+交换机 ==> 广播），内网
 
数据链路层，物理层给各个电脑连接起来了，还传输最底层的0和1电路信号，关键不行啊，你得定义清楚哪些0和1分为一组，这些信号啥意思？这才能进行通信。所以数据链路层就干这事儿，定义一下电路信号咋分组。
 
00000011（从电脑1出发，要到电脑2去）
 
 00101（从电脑1出发，要到电脑3去）
 
 0101（从电脑2触发，要到电脑4去）
 
 01（从电脑3出发，要到电脑5去）
 
很多年前，每个公司都定义自己的电路信号分组方式，但是后来出来了以太网协议，以太网。一组电信号是一个数据包，叫一个帧（frame），每个帧分成两个部分，标头（head）和数据（data），标头包含一些说明性的东西，比如说发送者、接收者和数据类型之类的。

以太网协议，每台电脑要往另外一台电脑发送数据，一堆0/1电路信号，封装成数据包，包含头和数据，头里包含了从哪儿来到哪儿去，必须从一台电脑的一个网卡，发送到另外一个电脑的一个网卡，所以以太网发送的数据包必须得指定，目标电脑的网卡的mac地址。
 
以太网规定了，每个网卡必须得包含一个mac地址，mac地址就是这个网卡的唯一标识，
 
以太网协议规定了，接入网络里的所有设备，都得有个网卡，以太网协议里的那个数据包，在数据链路层传输的数据包，必须从一个电脑的网卡传输到另外一个电脑的网卡，而这个网卡地址就叫做所谓的mac地址。每块网卡出厂的时候，就有一个唯一的mac地址，48位的二进制，但是一般用12个16进制数字表示，前6个16进制是厂商编号，后6个16进制是网卡流水号。
 
windows上，ipconfig /all，看看物理地址，就是mac地址，7C-67-A2-20-AB-5C
 
所以在以太网里传输数据包的时候，必须指定接收者的mac地址才能传输数据。
 
但是以太网的数据包怎么从一个mac地址发送到另一个mac地址？这个不是精准推送的，以太网里面，如果一个电脑发个数据包出去，会广播给局域网内的所有电脑设备的网卡，然后每台电脑都从数据包里获取接收者的mac地址，跟自己的mac地址对比一下，如果一样，就说明这是发给自己的数据包。
 
但是上面这种广播的方式，仅仅针对一个子网（局域网）内的电脑，会广播，否则一个电脑不能广播数据包给全世界所有的其他电脑吧，是仅仅广播给一个子网里面的电脑的。
如下图：
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-10-52-23.png width='100%'/></div><br/>

3）网络层（路由器也叫网关+ip协议+不断转发），外网
 
上面说到，子网内的电脑，通过以太网发个数据包，对局域网内的电脑，是广播出去的。那么怎么知道哪些电脑在一个子网内呢？这就得靠网络层了，这里就有一套IP地址，IP地址就可以让我们区分哪些电脑是一个子网的。
 
网络层里有IP协议，IP协议定义的地址就叫做IP地址。IP地址有IPv4和IPv6两个版本，目前广泛使用的是IPv4，是32个二进制数字组成的，但是一般用4个十进制数字表示，范围从0.0.0.0到255.255.255.255之间。
 
每台计算机，都会分配一个ip地址，ip地址的前24位（就是前面3个十进制数字），代表了网络，后8位（就是最后1个十进制数字），代表了主机。

如果几台电脑是一个子网的，那么前面的3个十进制数字一定是一样的。举个例子，大家平时做实验，玩儿虚拟机吧，自己win上开几个linux虚拟机，你会发现，win上的ip地址可能是192.168.0.103，然后几个虚拟机的ip地址是192.168.0.182，192.168.0.125，192.168.0.106，类似这样的。

这个win机器和几个虚拟机，前面3个十进制数字都是192.168.0，就代表大家是一个子网内的，最后那个数字是这个子网的不同主机的编号。
 
但是实际上上面就是举个例子，其实单单从ip地址是看不出来哪些机器是一个子网的，因为从10进制是判断不出来的。需要通过ip地址的二进制来判断，结合一个概念来判断，叫做子网掩码。

比如说ip地址是192.168.56.1，子网掩码是255.255.255.0。知道了子网掩码之后，如果要判断两个ip地址是不是一个子网的，就分别把两个ip地址和自己的子网掩码进行二进制的与运算，与运算之后，比较一下代表网络的那部分。
 
192.168.56.1和192.168.32.7，判断是不是一个子网的，拿子网掩码255.255.255.0，跟两个ip地址的二进制做与运算
 
11000000.10101000.00111000.00000001

11111111.11111111.11111111.00000000
 
子网掩码的二进制是：11111111.11111111.11111111.00000000，然后就跟ip地址的二进制做与好了，通过二进制来比较网络部分的地址是不是一模一样的。
 
有了网络层的ip地址之后，两台在子网内的电脑终于可以通过广播+mac地址判断来传输数据包进行通信了。
 
但是如果发现要接受数据包的计算机不在子网内，那么就不能通过广播来发送数据包，需要通过路由来发送数据包。

看到路由，就想到了路由器了，对了，路由器大家都熟悉吧，自己平时也会去买对吧，比如小米的路由器啥的，家里上网一般都会弄个路由器对吧，ok。路由器负责将多个子网进行连接，因为比如你在自己家里，其实你就只是你自己的一个子网，你要是访问网站啥的，是跟那个网站机器所在的子网进行通信。
 
每个电脑都可以搞多个网卡的，不是只有一个网卡，一般笔记本电脑都有以太网网卡和wifi网卡，发送数据包的时候要决定走哪个网卡。路由器，其实就是配置了多个网卡的一个专用设备，可以通过不同的网卡接入不同的网络。
 
网关其实是就是路由器的一种，运作在网络层，这个概念不多解释了，大家可以就把路由器上的ip地址认为是网关，路由器上每个网卡都有mac地址和对应的ip地址。路由器虽然有mac地址，但是不能通过mac地址寻址的，必须通过ip地址寻址，所以路由器其实是工作在网络层的设备。
 
网络交换机，也是一种设备，**网络交换机是工作在数据链路层的，路由器是工作在网路层**的。

网络交换机是通过mac地址来寻址和传输数据包的；但是路由器是通过ip地址寻址和传输数据包的。网络交换机主要用在局域网的通信，一般你架设一个局域网，里面的电脑通信是通过数据链路层发送数据包，通过mac地址来广播的，广播的时候就是通过网络交换机这个设备来把数据广播到局域网内的其他机器上去的；路由器一般用来让你连入英特网。
 
LAN，就是local area network，就是局域网；WAN，就是wide area network，就是广域网。WLAN是wireless local area network，就是无线局域网，也就是wifi，在局域网内，直接通过wifi无线联网。
 
**家里的路由器是包含了交换机和路由的两个功能的**，如果是连接到局域网内的设备就把线插LAN那儿；如果是连接到英特网，就把线插在WAN那儿。
 
这儿给大家举个例子，就是两个局域网之间，如果要是通过一个路由器进行通信的话，是怎么弄的。
 
大概过程就是，路由器配置了两块网卡，每个网卡可以连到一个局域网内。
 
局域网1内的电脑，要发送数据包到局域网2内的电脑，在数据包里写上自己的ip地址和对方的ip地址。但是他们俩不在一个局域网内，于是局域网1内的电脑，先通过交换机将数据包发送给路由器，这个过程需要将路由器的一块网卡的ip地址对应的mac地址写到数据包的头部，然后才能通过交换机广播出去，路由器接收到之后比较自己一块网卡的mac地址，就知道是来找自己的。
 
接着路由器接收到数据包之后，就会在局域网2内，将目标机器的ip地址对应的mac地址写入头部，接着再次通过交换机发送广播通知，发送给局域网2内的电脑。
 
一个局域网内的每台机器都有自己的ARP cache，这个ARP就是用来在一个局域网内让各个设备都知道每个设备的ip地址和mac地址的对应关系的，一般就是某个机器发送广播通知自己的ip地址和mac地址的对应关系，然后每个机器给他一个回应。以此类推，大家都互相这样广播一把，ip地址和mac地址的对应关系，大家不就都知道了吗？
 
所以大家在上面可以看到，一个子网内的机器之间通信，就是在数据包里写上对方的mac地址，然后交换机广播出去ok了；但是如果是跨子网的通信，就是写上对方的ip地址，然后先通过mac地址广播到路由器，让路由器再根据另外一个子网的ip地址转换为mac地址，通过另外一个子网的交换机广播过去。就这个意思。
如图：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-10-54-24.png width='100%'/></div><br/>

4）传输层（tcp协议/udp协议 ==> 应用程序的端口号）
 
上面我们大概明白了通过网络层的ip地址怎么划分出来一个一个的子网，然后在子网内部怎么通过mac地址广播通信；跨子网的时候，怎么通过ip地址 -> mac地址 -> 交换机 -> 路由器 -> ip地址 -> mac地址 -> 交换机的方式来通过路由器进行通信。
 
但是这里还有一个问题，就是一台机器上，是很多个程序用一个网卡进行网络通信的，比如说浏览器、QQ、视频直播，这些软件都用了一个网卡往外面发送数据，然后从网卡接收数据，对吧。
 
所以还需要一个端口号的概念，就是你得发送数据包到某个机器的一个网卡的某个端口上去，然后那个机器上监听那个端口的程序，就可以提取发送到这个端口的数据，知道是自己的数据。端口号是0~65536的范围内，0~1023被系统占用了，别的应用程序就用1024以上的端口就ok了。
 
电脑1，是在端口48362监听的，通过网卡发送了一条数据 -> 电脑2的ip地址的20386这个端口 -> 电脑2的上面的某个QQ，监听着20386的端口 -> 电脑2的网卡接收到一条数据之后，发现人家找的是20386这个端口，就去找谁哪个哥儿们在监听20386端口，QQ在监听，我就把这个网卡过来的数据，传递给QQ，通过端口知道，哪条数据是给你的
 
所以其实大家会发现一点，网络层，是基于ip协议，进行主机和主机间的寻址和通信的，然后传输层，其实是建立某个主机的某个端口，到另外一个主机的某个端口的连接和通信的。

这个通信，就是通过socket来实现的，通过socket就可以基于tcp/ip协议完成刚才上面说的一系列的比如基于ip地址和mac地址转换和寻址啊，通过路由器通信啊之类的，而且会建立一个端口到另外一个端口的连接。
 
udp和tcp都是传输层的协议，作用就是在数据包里加入端口号，可以通过端口号进行点对点的通信了。udp协议是不可靠的，发出去人家收到没有就不知道了；tcp协议是可靠的，要求三次握手，而且要求人家接收到数据必须回复你。
 
传输层的tcp协议，仅仅只是规定了一套基于端口的点对点的通信协议，包括如何建立连接，如何发送和读取消息，但是实际上如果你要基于tcp协议来开发，你一般是用socket，java socket网络编程， 如下图：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-10-55-54.png width='100%'/></div><br/>

5）应用层(http协议)

通过传输层的tcp协议可以传输数据，但是人家收到数据之后，怎么来解释？比如说收到个邮件你怎么处理？收到个网页你怎么处理？类似这个意思，所以针对各种不同的应用，邮件、网页之类的，都是定义不同的应用层协议的。这个应用层，我们就假设综合了会话层、表示层和应用层了，3层合成1层。

电脑1走tcp协议发送了一段东西过来，发送到电脑2的20386端口

GET http://localhost:8080/ http/1.1

key:valuel
key:value

电脑2走tcp协议读取到了属于自己这个20386端口 的一段数据

GET http://localhost:8080/ http/1.1

key:valuel
key:value

发送了一段响应

200

key;value
key:value

又通过底层的tcp发了出去，电脑1的30987端口，ip

电脑1，网卡，走以太网协议收到一个数据包

200

key;value
key:value

比如最常见的，应用层的协议就是http协议，进行网络通信。

然后我们看下自己的网络设置，一般包含了ip地址、子网掩码、网关地址、DNS地址。前面3个我们其实都知道啥意思了。ip地址和子网掩码用来划分子网的，判断哪些ip地址在一个子网内。同时你的ip地址和mac地址关联起来的，唯一定位了你的网卡。网关地址，你就认为是路由器上的那个网卡的ip地址吧，路由器的网卡也有mac地址，mac地址对应了一个ip地址。

DNS地址是啥呢？Domain Name System。因为我们一般定位是通过ip地址+mac地址+端口号来定位一个通信目标的，但是如果在浏览器上输入一个www.baidu.com，咋整？这个时候是先把www.baidu.com发给DNS服务器，然后DNS服务器告诉你www.baidu.com对应的ip地址的。

6）总结

4层：数据链路层（以太网协议），网络层（ip协议），传输层（tcp协议），应用层（http协议）。

7层：物理层（网线，光纤，传递0/1电路信号）， 而应用层包括：会话层，表示层，应用层。

自己的ip必须和路由器（网关）是在同一个子网内，才能访问外网。

## 47-48、浏览器请求www.baidu.com的全过程大概是怎么样的？（上 下）

如果你阅读过昨天发布文章，就应该知道网络七层模型大概都是怎么回事了，然后四层模型其实就是会话层、表示层和应用层，合并为了一个应用层，同时没把物理层算在内

并且我们也大概知道每一层的协议和作用，网络通信的时候都是怎么回事了，现在我们来看看假设通过浏览器发送一个请求，你访问到那个网站对应的机器，然后人家再给你一个响应的全过程。

现在我们先假设，我们给电脑设置了几个东西：
ip地址：192.168.31.37
子网掩码：255.255.255.0
网关地址：192.168.31.1
DNS地址：8.8.8.8

这时，我们打开一个浏览器，请求www.baidu.com地址，这个时候找DNS服务器，DNS服务器解析域名之后，返回一个ip地址，比如172.194.26.108。

接着会判断两个ip地址是不是一个子网的，用子网掩码255.255.255.0，对两个ip地址做与运算，拿到192.168.31.0和172.194.26.0，明显不是一个子网的。

如图：
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-07-51.png width='100%'/></div><br/>

那就得发送一个数据包给网关，其实你就认为是我们的路由器吧，就是192.168.31.1，而且我们是可以拿到网关ip地址的mac地址的，现在我们从应用层出发，通过浏览器访问一个网站，是走应用层的http协议的，并且要把浏览器发出的请求打包成数据包，要把哪些东西给放到数据包中去呢？

http协议分为几个部分：
请求方法+URL地址+http版本

比如
GEThttp://172.194.26.108/testHTTP/1.1，类似这种请求头，类似下面这种：
Host:upload.jiangsu.io
Proxy-Connection:keep-alive
User-Agent:Mozilla/5.0
等等。。。

请求体，比如常见的可以放一个json这就构成了一个http请求报文浏览器请求一个地址，先按照应用层的http协议，封装一个应用层数据包，数据包里就放了http请求报文，这个时候会将这个http请求报文打包成一个数据包，仅仅只是数据包的数据部分，此时是数据包是没有头的。上面根据http协议搞一个http请求报文，然后搞一个数据包出来，就是网络模型中到的应用层干的事儿了。

接着就是跑传输层来了，这个层是tcp协议，这个tcp协议会让你设置端口，发送方的端口随机选一个，接收方的端口一般是默认的80端口。

这个时候，会把应用层数据包给封装到tcp数据包中去，而且会加一个tcp头，这个tcp数据包是对应一个tcp头的，这个tcp头里就放了端口号信息。

如图：
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-09-03.png width='100%'/></div><br/>

接着跑到网络层来了，走ip协议，这个时候会把tcp头和tcp数据包，放到ip数据包里去，然后再搞一个ip头，ip头里本机和目标机器的ip地址。

这里本机ip地址是192.168.31.37，
目标机器是172.194.26.108。

因为，通过ip协议，可以判断说，两个ip地址不是在一个子网内的，所以此时只能将数据包先通过以太网协议广播到网关上去，通过网关再给他发送出去。

如图：
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-10-49.png width='100%'/></div><br/>

接着是数据链路层，这块走以太网协议，这里是把ip头和ip数据包封到以太网数据包里去，然后再加一个以太网数据包的头，头里放了本机网卡mac地址，和网关的mac地址。但是以太网数据包的限制是1500个字节，但是假设这个时候ip数据包都5000个字节了，那么需要将ip数据包切割一下。

这个时候一个以太网数据包要切割为4个数据包，每个数据包包含了以太网头、ip头和切割后的ip数据包，4个数据包的大小分别是1500，1500,1500，560。ip头里包含了每个数据包的序号。

如图：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-11-30.png width='100%'/></div><br/>

这4个以太网数据包都会通过交换机发到你的网关上，然后你的路由器是可以联通别的子网的，这个是时候你的路由器就会转发到别的子网的可能也是某个路由器里去，然后以此类推吧，N多个路由器或者你叫网关也行，N多个网关转发之后，就会跑到百度的某台服务器，接收到4个以太网数据包。

百度服务器接收到4个以太网数据包以后，根据ip头的序号，把4个以太网数据包里的ip数据包给拼起来，就还原成一个完整的ip数据包了。接着就从ip数据包里面拿出来tcp数据包，再从tcp数据包里取出来http数据包，读取出来http数据包里的各种协议内容，接着就是做一些处理，然后再把响应结果封装成htp响应报文，封装在http数据包里，再一样的过程，封装tcp数据包，封装ip数据包，封装以太网数据包，接着通过网关给发回去。

如下图：
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-12-50.png width='100%'/></div><br/>

## 49、画一下TCP三次握手流程图？为啥是三次而不是二次或者四次呢？

1、面试题

TCP三次握手和四次握手的工作流程是什么（画一下流程图）？为什么不是五次握手或者两次握手？

2、面试官心里分析

这个问题相当经典，大家可别以为就是考察应届生的，实际上在普通社招java面试中，一些大公司，很喜欢考察这个问题，尤其是后面第二个追加问题，让你聊聊为啥必须是三次握手，而不是两次呢？

3、面试题剖析

（1）tcp三次握手过程

通过传输层的tcp协议建立网络连接的时候，其实走的是三次握手的过程

建立三次握手的时候，TCP报头用到了下面几个东西，ACK、SYN、FIN。

第一次握手，客户端发送连接请求报文，此时SYN=1、ACK=0，这就是说这是个连接请求，seq = x，接着客户端处于SYN_SENT状态，等待服务器响应。

第二次握手，服务端收到SYN=1的请求报文，需要返回一个确认报文，ack = x + 1，SYN=1，ACK = 1，seq = y，发送给客户端，自己处于SYN_RECV状态。

第三次握手，客户端收到了报文，将ack = y + 1，ACK = 1，seq = x + 1

其实三次握手说白了，就是来回来去三次请求，每次请求带上一堆TCP报文头，根据报文头是否正确，就是越好的协议来建立连接。简单说就是这样。
<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-21-05.png width='100%'/></div><br/>

（2）为啥不是2次或者4次握手呢？

假设两次握手就ok了，要是客户端第一次握手过去，结果卡在某个地方了，没到服务端；完了客户端再次重试发送了第一次握手过去，服务端收到了，ok了，大家来回来去，三次握手建立了连接。

结果，尴尬的是，后来那个卡在哪儿的老的第一次握手发到了服务器，服务器直接就返回一个第二次握手，这个时候服务器开辟了资源准备客户端发送数据啥的，结果呢？客户端根本就不会理睬这个发回去的二次握手，因为之前都通信过了。

但是如果是三次握手，那个二次握手发回去，客户端发现根本不对，就会发送个复位的报文过去，让服务器撤销开辟的资源，别等着了。

因为3次握手就够了，不需要4次或者5次浪费资源了。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-21-56.png width='100%'/></div><br/>

（3）tcp断开连接的4次挥手

第一次挥手，客户端发送报文，FIN=1，seq=u，此时进入FIN-WAIT-1状态

第二次挥手，服务端收到报文，这时候进入CLOSE_WATI状态，返回一个报文，ACK=1，ack=u+1，seq=v。客户端收到这个报文之后，直接进入FIN-WAIT-2状态，此时客户端到服务端的连接就释放了。

第三次挥手，服务端发送连接释放报文，FIN=1，ack=u+1，seq=w，服务端进入LAST-ACK状态

第四次挥手，客户端收到连接释放报文之后，发应答报文，ACK=1，ack=w+1，seq=u+1，进入TIME_WAIT状态，等待一会儿客户端进入CLOSED状态，服务端收到报文之后就进入CLOSED状态。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-22-35.png width='100%'/></div><br/>

## 50、聊聊HTTP协议的工作原理！

1、面试题

说一下http的工作流程？http 1.0、http 1.1、http 2.0具体有哪些区别？

2、面试官心里分析

这个就是让你聊聊http，说白了http工作原理，你都知道了，发起个http，底层都是tcp、ip、以太网那块再走，一层一层包裹数据包。所以http的关键就是让你聊聊http请求和http响应的规范。

3、面试题剖析

http发起请求的底层原理，大家其实都知道了，理解了那个原理，就一通百通了。那么来聊下http请求和响应的规范吧。其实请求的报文，就是请求头、请求方法、请求正文，GET/POST啥的，应该都知道；请求头，自己百度一下吧，作为一个工程师必须知道。响应，状态行，响应头，响应正文，状态行，200,400,500，实在不想讲了；响应头，自己查一下。

http请求封装到应用层数据包，封装在tcp数据包，封装在ip数据包，封装在以太网数据包，如果过大，可能会拆成几个包，走以太网协议+交换机 -> 广播 -> 网关 -> 多个网关 -> 目标的机器 -> 一层一层拆包 -> http请求报文 -> 传递给tomcat -> spring mvc -> http响应 -> 一样的路径会去

最最底层，这个数据如何传输？走的是物理层，网线、光缆，所有数据都是0/1电路信号

http协议，其实是每个搞java必须会的基础。

互联网初期，一般一个网页几乎都没什么图片，当时就是挂一些文字，一个网页里就是一大坨的文字。http 1.0版本。

浏览器 -> 网站，互相之间是先要通过tcp三次握手，建立一个连接，浏览器和网站互相都给对方留出一份资源，浏览器发起http请求 -> tcp -> ip -> 以太网，网站上面去，网站返回一个响应，连接关闭，tcp四次挥手。释放掉浏览器和网站各自给对方保持的一份资源。

http 1.0要指定keep-alive来开启持久连接，默认是短连接，就是浏览器每次请求都要重新建立一次tcp连接，完事儿了就释放tcp连接。早期的网页都很low，没啥东西，就一点文字，就用这个没问题。但是现在，一个网页打开之后，还要加载大量的图片、css、js，这就坑爹了，发送多次请求。

早期，2000年之前，那个时候网页，都很low，当时你打开一个网页，就是说现场底层tcp三次握手，跟网站建立一个tcp连接，然后通过这个tcp连接，发送一次http请求，网站返回一个http响应（网页的html，里面有一大段文字），浏览器收到html渲染成网页，浏览器就走tcp四次挥手，跟网站断开连接了

到了后面，发现说2000之后，2010之后更不用说了，网页发展很迅猛，一个网页包含着大量的css、js、图片等资源。比如你请求一个网页，这个网页的html先过来，过来之后，浏览器再次发起大量的请求去加载css、js、图片，打开一个网页可能浏览器要对网站服务器发送几十次请求。

http 1.0，疯了，刚开始请求网页的html，tcp三次握手建立连接 -> 请求/响应 -> tcp四次挥手断开连接，接着再次要加载css、js、图片，要发送30个请求，上面的过程来30次，30次频繁的建立tcp连接以及释放tcp连接。很慢很慢。

其实最慢的不是说发送请求和获取响应，打开和释放连接，这都是很重的过程

http 1.1默认支持长连接，就是说，浏览器打开一个网页之后，底层的tcp连接就保持着，不会立马断开，之后加载css、js之类的请求，都会基于这个tcp连接来走。http 1.1还支持host头，也就可以支持虚拟主机；而且对断点续传有支持。

浏览器，第一次请求去一个网站的一个页面的时候，就会打开一个tcp连接，接着就在一段时间内都不关闭了，然后接下来这个网页加载css、js、图片大量的请求全部走同一个tcp连接，频繁的发送请求获取响应，最后过了一段时间，这些事儿都完了，然后才会去释放那一个tcp连接。大幅度的提升复杂网页的打开的速度，性能。

http 2.0，支持多路复用，基于一个tcp连接并行发送多个请求以及接收响应，解决了http 1.1对同一时间同一个域名的请求有限制的问题。二进制分帧，将传输数据拆分为更小的帧（数据包），frame（数据包，帧），提高了性能，实现低延迟高吞吐。

## 51、聊聊HTTPS的工作原理？为啥用HTTPS就可以加密通信？

1、面试题

http和https的区别是什么？https的原理是什么？

2、面试官心理分析

聊到http了，那肯定会聊聊https

3、面试题剖析

http协议都是明文的，是没有加密的，所以其实现在一般大部分应用都是用https协议的。之前是基于SSL协议对http进行加密，后来又升级到了TSL协议来加密，现在称之为SSL / TSL吧。

https的工作原理大概是这样的：

（1）浏览器把自己支持的加密规则发送给网站

（2）网站从这套加密规则里选出来一套加密算法和hash算法，然后把自己的身份信息用证书的方式发回给浏览器，证书里有网站地址、加密公钥、证书颁发机构

（3）浏览器验证证书的合法性，然后浏览器地址栏上会出现一把小锁；浏览器接着生成一串随机数密码，然后用证书里的公钥进行加密，这块走的非对称加密；用约定好的hash算法生成握手消息的hash值，然后用密码对消息进行加密，然后把所有东西都发给网站，这块走的是对称加密

（4）网站，从消息里面可以取出来公钥加密后的随机密码，用本地的私钥对消息解密取出来密码，然后用密码解密浏览器发来的握手消息，计算消息的hash值，并验证与浏览器发送过来的hash值是否一致，最后用密码加密一段握手消息，发给浏览器

（5）浏览器解密握手消息，然后计算消息的hash值，如果跟网站发来的hash一样，握手就结束，之后所有的数据都会由之前浏览器生成的随机密码，然后用对称加密来进行进行加密。

常用的非对称加密是RSA算法，对称加密是AES、RC4等，hash算法就是MD5

就好比，有个人说我加密的时候是用了一个公钥去加密，然后你解密的时候是用私钥去解密；我加密的时候用的算法，跟解密的时候用的算法，是一样的，对称加密

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-22-11-28-38.png width='100%'/></div><br/>

52、聊聊http的长连接的工作原理到底是啥？

1、面试题

什么是长连接？http长连接是什么？

2、面试官心里分析

一期学员，在外面面试的时候，正好还碰到了，聊到dubbo，dubbo://协议，是走的长连接，你聊聊什么是长连接？什么是http长连接？

3、面试题剖析

http本身没什么所谓的长连接短连接之说，其实说白了都是http下层的tcp连接是长连接还是短连接，tcp连接保持长连接，那么多个http请求和响应都可以通过一个链接来走。其实http 1.1之后，默认都是走长连接了，就是底层都是一个网页一个tcp连接，一个网页的所有图片、css、js的资源加载，都走底层一个tcp连接，来多次http请求即可。

http 1.0的时候，底层的tcp是短连接，一个网页发起的请求，每个请求都是先tcp三次握手，然后发送请求，获取响应，然后tcp四次挥手断开连接；每个请求，都会先连接再断开。短连接，建立连接之后，发送个请求，直接连接就给断开了

http 1.1，tcp长连接，tcp三次握手，建立了连接，无论有多少次请求都是走一个tcp连接的，走了n多次请求之后，然后tcp连接被释放掉了

## 53-54、MySQL、MyISAM和InnoDB存储引擎的区别是啥？（上 下）
1、面试题

MySQL有哪些存储引擎啊（myisam和innodb）？都有什么区别？请详细说明一下。

2、面试官心里分析

这个其实说实话也是聊MySQL必备的问题，我已经在指导我们架构班的同学在外面跳槽了，根据大家的反馈来看，我觉得确实是，数据库这块还是经常问的，确实可以看到，20多k的职位的话，这块问的不会太深的，就是问问常规的一些问题。

3、面试题剖析

mysql支持的存储引擎有很多种，innodb、myisam、memory，很多，但是我就讲其中两种，因为其实现在，常用的就一种，innodb，myisam以前可能还有一些场景会用，现在用的已经非常少了

（1）myisam

myisam，不支持事务，不支持外键约束，索引文件和数据文件分开，这样在内存里可以缓存更多的索引，对查询的性能会更好，适用于那种少量的插入，大量查询的场景。

比如说最经典的就是报表系统，比如大数据的报表系统，给大家画个图聊聊一半都是怎么玩儿的，常见的就是走hadoop生态来搞，hdfs来存储数据，然后基于hive来进行数仓建模，每次hive跑出来的数据都用sqoop从hive中导出到mysql中去。然后基于mysql的在线查询，就接上j2ee写个简单的web系统，每个报表开发一套代码，写sql查数据，组织数据，按照前端要求的格式返回数据，展现出来一个报表。

这种报表系统，是最适合mysql的myisam存储引擎的，不需要事务，就是一次性批量导入，接下来一天之内就是纯查询了。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-13-56-59.png width='100%'/></div><br/>

这个是比较low的做法，说实在的，现在你要让我说myisam的场景其实不多了，在很多大数据场景里是不适用的，因为真正的大数据系统，很多时候hadoop跑出来的结果还是很大，一天就几千万结果数据，几十亿明细数据，那mysql是抗不住这么大量的数据的。所以现在大数据一般用kylin做离线数据的分析引擎，直接hive数据导入kylin里面去了，或者也可以走elasticsearch。

尝试过做过一个事情，用mysql分库分表来抗，抗不住了，单表一般建议是控制在几百万的数据量级，500w以内的数据量，多少表？多少库？多少台数据库服务器？sql多达几百行，各种子查询、join、函数、行转列、列传行，非常不适合用mysql -> 数据量很大 -> sql很复杂 -> 导致mysql数据库服务器cpu负载过高

比较高端一点了，我们会基于自己研发的可配置化BI系统 + kylin + elasticsearch，支持大规模数据的复杂报表的支持，做的非常好，效果远远超出基于mysql的那套方案

后来还有那种实时数据报表，就是storm或者是spark streaming，跑数据出来，来一条算一条，然后结果立马写入mysql中，这个的话，一般就保留当天数据，其实压力不会太大，但是问题在于说，可能写并发会超高，每秒并发轻易就可以几千甚至上万。所以大数据实时报表不会写mysql了，现在一般都是写es。

你可以按照我上面的这套说辞去说说，如果是java方向的同学，就说你们之前配合你们公司的数据团队开发过这种报表系统的j2ee部分，所以当时用myisam比较多，但是后来人家几乎都不用了，借此体现出你是有实际经验的，这回答的档次都不一样了。

（2）innodb

说真的，现在一般用mysql都是innodb，我真很少用其他的存储引擎，而且国内用其他存储引擎的场景和公司也不多，所以用innodb就可以了，而且这个也是mysql 5.5之后的默认存储引擎。

主要特点就是支持事务，走聚簇索引，强制要求有主键，支持外键约束，高并发、大数据量、高可用等相关成熟的数据库架构，分库分表、读写分离、主备切换，全部都可以基于innodb存储引擎来玩儿，如果真聊到这儿，其实大家就可以带一带，说你们用innodb存储引擎怎么玩儿分库分表支撑大数据量、高并发的，怎么用读写分离支撑高可用和高并发读的，用上第1季的内容就可以了。

说实话，关于存储引擎，现在因为其实真的主要就是innodb，聊到这儿就可以了，反而被问到这问题，多拓展根据你的经验来回答

## 55-56、聊聊MySQL的索引实现原理？各种索引你们平时都怎么用的？（上下）

1、面试真题

+ MySQ索引的原理和数据结构能介绍一下吗?
+ b+树和b-树有什么区别？
+ MySQL聚簇索引和非聚簇索引的区别是什么？
+ 他们分别是如何存储的？
+ 使用MySQL索引都有哪些原则？
+ MySQL复合索引如何使用？

2、面试官心理分析
数据库是30k以内的工程师面试必问的问题，而且如果问数据库，一定是问mysql，N年前可能java工程师出去面试，oracle这块的技能是杀手锏，现在已经没人说，会oracle是加分项了，现在都是熟悉大数据hadoop、hbase等技术是加分项。

3、面试题剖析

3.1 索引的数据结构是什么

其实就是让你聊聊mysql的索引底层是什么数据结构实现的，弄不好现场还会让你画一画索引的数据结构，然后会问问你mysql索引的常见使用原则，弄不好还会拿个SQL来问你，就这SQL建个索引一般咋建？
至于索引是啥？这个问题太基础了，大家都知道，mysql的索引说白了就是用一个数据结构组织某一列的数据，然后如果你要根据那一列的数据查询的时候，就可以不用全表扫描，只要根据那个特定的数据结构去找到那一列的值，然后找到对应的行的物理地址即可。
那么回答面试官的一个问题，mysql的索引是怎么实现的？

答案是，不是二叉树，也不是一颗乱七八糟的树，而是一颗b+树。这个很多人都会这么回答，然后面试官一定会追问，那么你能聊聊b+树吗？

但是说b+树之前，咱们还是先来聊聊b-树是啥，从数据结构的角度来看，b-树要满足下面的条件：

- （1）d为大于1的一个正整数，称为B-Tree的度。
- （2）h为一个正整数，称为B-Tree的高度。
- （3）每个非叶子节点由n-1个key和n个指针组成，其中d<=n<=2d。
- （4）每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。
- （5）所有叶节点具有相同的深度，等于树高h。
- （6）key和指针互相间隔，节点两端是指针。
- （7）一个节点中的key从左到右非递减排列。
- （8）所有节点组成树结构。
- （9）每个指针要么为null，要么指向另外一个节点。
- （10）如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。
- （11）如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。
- （12）如果某个指针在节点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)且大于v(keyi)。

 上面那段规则，我也是从网上找的，说实话，没几个java程序员能耐心去看明白或者是背下来，大概知道是个树就好了。就拿个网上的图给大家示范一下吧：
 比如说我们现在有一张表：

```sql
(
id int
name varchar
age int
)
-- 我们现在对id建个索引：15、56、77、20、49
select * from table where id = 49
select * from table where id = 15
```

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-14-58-46.png width='100%'/></div><br/>

反正大概就长上面那个样子，查找的时候，就是从根节点开始二分查找。大概就知道这个是事儿就好了，深讲里面的数学问题和算法问题，时间根本不够，面试官也没指望你去讲里面的数学和算法问题，因为我估计他自己也不一定能记住。
好了，b-树就说到这里，直接看下一个，b+树。b+树是b-树的变种，啥叫变种？就是说一些原则上不太一样了，稍微有点变化，同样的一套数据，放b-树和b+树看着排列不太一样的。而mysql里面一般就是b+树来实现索引，所以b+树很重要。

b+树跟b-树不太一样的地方在于：

- 每个节点的指针上限为2d而不是2d+1。
- 内节点不存储data，只存储key；
- 叶子节点不存储指针。

这图我就不自己画了，网上弄个图给大家瞅一眼：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-15-00-57.png width='100%'/></div><br/>

```sql
select * from table where id = 15
select * from table where id>=18 and id<=49
```

但是一般数据库的索引都对b+树进行了优化，加了顺序访问的指针，如网上弄的一个图，这样在查找范围的时候，就很方便，比如查找18~49之间的数据：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-15-05-39.png width='100%'/></div><br/>

其实到这里，你就差不多了，你自己仔细看看上面两个图，b-树和b+树都现场画一下，然后给说说区别，和通过b+树查找的原理即可。

接着来聊点稍微高级点的，因为上面说的只不过都是最基础和通用的b-树和b+树罢了，但是mysql里不同的存储引擎对索引的实现是不同的。

3.2 myism存储引擎的索引实现

先来看看myisam存储引擎的索引实现。就拿上面那个图，咱们来现场手画一下这个myisam存储的索引实现，在myisam存储引擎的索引中，每个叶子节点的data存放的是数据行的物理地址，比如0x07之类的东西，然后我们可以画一个数据表出来，一行一行的，每行对应一个物理地址。

索引文件

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-15-25-39.png width='100%'/></div><br/>

id=15，data：0x07，0a89，数据行的物理地址

数据文件单独放一个文件

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-15-26-25.png width='100%'/></div><br/>

select * from table where id = 15 -> 0x07物理地址 -> 15，张三，22

myisam最大的特点是数据文件和索引文件是分开的，大家看到了么，先是索引文件里搜索，然后到数据文件里定位一个行的。

**3.3 innodb存储引擎的索引**

好了，再来看看innodb存储引擎的索引实现，跟myisam最大的区别在于说，innodb的数据文件本身就是个索引文件，就是主键key，然后叶子节点的data就是那个数据的所在行。我们还是用上面那个索引起来现场手画一下这个索引好了，给大家来感受一下。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-15-27-58.png width='100%'/></div><br/>

innodb存储引擎，要求必须有主键，会根据主键建立一个默认索引，叫做聚簇索引，innodb的数据文件本身同时也是个索引文件，索引存储结构大致如下：

15，data：0x07，完整的一行数据，（15,张三,22）

22，data：完整的一行数据，（22,李四,30）

就是因为这个原因，innodb表是要求必须有主键的，但是myisam表不要求必须有主键。另外一个是，innodb存储引擎下，如果对某个非主键的字段创建个索引，那么最后那个叶子节点的值就是主键的值，因为可以用主键的值到聚簇索引里根据主键值再次查找到数据，即所谓的回表，例如：

```select * from table where name = ‘张三’;```

先到name的索引里去找，找到张三对应的叶子节点，叶子节点的data就是那一行的主键，id=15，然后再根据id=15，到数据文件里面的聚簇索引（根据主键组织的索引）根据id=15去定位出来id=15这一行的完整的数据

所以这里就明白了一个道理，为啥innodb下不要用UUID生成的超长字符串作为主键？因为这么玩儿会导致所有的索引的data都是那个主键值，最终导致索引会变得过大，浪费很多磁盘空间。

还有一个道理，一般innodb表里，建议统一用auto_increment自增值作为主键值，因为这样可以保持聚簇索引直接加记录就可以，如果用那种不是单调递增的主键值，可能会导致b+树分裂后重新组织，会浪费时间。

**3.4 索引的使用规则**

一般来说跳槽时候，索引这块必问，b+树索引的结构，一般是怎么存放的，出个题，针对这个SQL，索引应该怎么来建立

```select * from table where a=1 and b=2 and c=3;```你知道不知道，你要怎么建立索引，才可以确保这个SQL使用索引来查询

好了，各位同学，聊到这里，你应该知道具体的myisam和innodb索引的区别了，同时也知道什么是聚簇索引了，现场手画画，应该都ok了。然后我们再来说几个最最基本的使用索引的基本规则。

其实最基本的，作为一个java码农，你得知道最左前缀匹配原则，这个东西是跟联合索引（复合索引）相关联的，就是说，你很多时候不是对一个一个的字段分别搞一个一个的索引，而是针对几个索引建立一个联合索引的。

给大家举个例子，你如果要对一个商品表按照店铺、商品、创建时间三个维度来查询，那么就可以创建一个联合索引：shop_id、product_id、gmt_create

一般来说，你有一个表（product）：shop_id、product_id、gmt_create，你的SQL语句要根据这3个字段来查询，所以你一般来说不是就建立3个索引，一般来说会针对平时要查询的几个字段，建立一个联合索引

后面在java系统里写的SQL，都必须符合最左前缀匹配原则，确保你所有的sql都可以使用上这个联合索引，通过索引来查询

 ```create index (shop_id,product_id,gmt_create)```

 （1）全列匹配

这个就是说，你的一个sql里，正好where条件里就用了这3个字段，那么就一定可以用到这个联合索引的：

```select * from product where shop_id=1 and product_id=1 and gmt_create=’2018-01-01 10:00:00’;```

（2）最左前缀匹配

这个就是说，如果你的sql里，正好就用到了联合索引最左边的一个或者几个列表，那么也可以用上这个索引，在索引里查找的时候就用最左边的几个列就行了：

```select * from product where shop_id=1 and product_id=1;```这个是没问题的，可以用上这个索引的

（3）最左前缀匹配了，但是中间某个值没匹配

这个是说，如果你的sql里，就用了联合索引的第一个列和第三个列，那么会按照第一个列值在索引里找，找完以后对结果集扫描一遍根据第三个列来过滤，第三个列是不走索引去搜索的，就是有一个额外的过滤的工作，但是还能用到索引，所以也还好，例如：

```select * from product where shop_id=1 and gmt_create=’2018-01-01 10:00:00’;```

就是先根据shop_id=1在索引里找，找到比如100行记录，然后对这100行记录再次扫描一遍，过滤出来gmt_create=’2018-01-01 10:00:00’的行

这个我们在线上系统经常遇到这种情况，就是根据联合索引的前一两个列按索引查，然后后面跟一堆复杂的条件，还有函数啥的，但是只要对索引查找结果过滤就好了，根据线上实践，单表几百万数据量的时候，性能也还不错的，简单SQL也就几ms，复杂SQL也就几百ms。可以接受的。

（4）没有最左前缀匹配

那就不行了，那就在搞笑了，一定不会用索引，所以这个错误千万别犯

```select * from product where product_id=1;```**这个肯定不行。**

（5）前缀匹配

这个就是说，如果你不是等值的，比如=，>=，<=的操作，而是like操作，那么必须要是like ‘XX%’这种才可以用上索引，比如说

```sql
select * from product where shop_id=1 and product_id=1 and gmt_create like ‘2018%’;
```

（6）范围列匹配

如果你是范围查询，比如>=，<=，between操作，你只能是符合最左前缀的规则才可以范围，范围之后的列就不用索引了

```sql
select * from product where shop_id>=1 and product_id=1;
```

这里就在联合索引中根据shop_id来查询了

（7）包含函数

如果你对某个列用了函数，比如substring之类的东西，那么那一列不用索引

```sql
select * from product where shop_id=1 and 函数(product_id) = 2
```

上面就根据shop_id在联合索引中查询

3.5 索引的缺点以及使用注意

索引是有缺点的，比如常见的就是会增加磁盘消耗，因为要占用磁盘文件，同时高并发的时候频繁插入和修改索引，会导致性能损耗的。

我们给的建议，尽量创建少的索引，比如说一个表一两个索引，两三个索引，十来个，20个索引，高并发场景下还可以。

字段，status，100行，status就2个值，0和1
你觉得你建立索引还有意义吗？几乎跟全表扫描都差不多了

```select * from table where status=1，```

相当于是把100行里的50行都扫一遍
你有个id字段，每个id都不太一样，建立个索引，这个时候其实用索引效果就很好，你比如为了定位到某个id的行，其实通过索引二分查找，可以大大减少要扫描的数据量，性能是非常好的

在创建索引的时候，要注意一个选择性的问题，```select count(discount(col)) / count(*)，```就可以看看选择性，就是这个列的唯一值在总行数的占比，如果过低，就代表这个字段的值其实都差不多，或者很多行的这个值都类似的，那创建索引几乎没什么意义，你搜一个值定位到一大坨行，还得重新扫描。

就是要一个字段的值几乎都不太一样，此时用索引的效果才是最好的

还有一种特殊的索引叫做前缀索引，就是说，某个字段是字符串，很长，如果你要建立索引，最好就对这个字符串的前缀来创建，比如前10个字符这样子，要用前多少位的字符串创建前缀索引，就对不同长度的前缀看看选择性就好了，一般前缀长度越长选择性的值越高。

好了，各位同学，索引这块能聊到这个程度，或者掌握到这个程度，其实普通的互联网系统中，80%的活儿都可以干了，因为在互联网系统中，一般就是尽量降低SQL的复杂度，让SQL非常简单就可以了，然后搭配上非常简单的一个主键索引（聚簇索引）+ 少数几个联合索引，就可以覆盖一个表的所有SQL查询需求了。更加复杂的业务逻辑，让java代码里来实现就ok了。

大家要明白，SQL达到95%都是单表增删改查，如果你有一些join等逻辑，就放在java代码里来做。SQL越简单，后续迁移分库分表、读写分离的时候，成本越低，几乎都不用怎么改造SQL。
我这里给大家说下，互联网公司而言，用MySQL当最牛的在线即时的存储，存数据，简单的取出来；不要用MySQL来计算，不要写join、子查询、函数放MySQL里来计算，高并发场景下；计算放java内存里，通过写java代码来做；可以合理利用mysql的事务支持。

## 57-58、你能说说事务的几个特性是啥？有哪几种隔离级别？（上下）

1、面试题

- 事务的几个特点是什么？
- 数据库事务有哪些隔离级别？
- MySQL的默认隔离级别？

2、面试官心里分析

用mysql开发的三个基本面：存储引擎、索引，然后就是事务，你必须得用事务。

因为一个业务系统里，肯定要加事务保证一堆关联操作，要么一起成功要么一起失败，对不对？所以这是聊数据库必问的一个问题

最最最基本的用mysql来开发，就3点：存储引擎（了解），索引（能建索引，写的SQL都用上索引），事务（了解事务的隔离级别，基于spring的事务支持在代码里加事务）

存储引擎 -> innodb，索引，基本按照你的SQL的需求都建了索引（可能漏了部分索引忘了建），事务（@Transactional注解，对service层统一加了事务）

3、面试题剖析

3.1 事务的ACID

这个先说一下ACID，必须得知道：

（1）Atomic：原子性，就是一堆SQL，要么一起成功，要么都别执行，不允许某个SQL成功了，某个SQL失败了，这就是扯淡，不是原子性。

（2）Consistency：一致性，这个是针对数据一致性来说的，就是一组SQL执行之前，数据必须是准确的，执行之后，数据也必须是准确的。别搞了半天，执行完了SQL，结果SQL对应的数据修改没给你执行，那不是坑爹么。

（3）Isolation：隔离性，这个就是说多个事务在跑的时候不能互相干扰，别事务A操作个数据，弄到一半儿还没弄好呢，结果事务B来改了这个数据，导致事务A的操作出错了，那不就搞笑了。

（4）Durability：持久性，事务成功了，就必须永久对数据的修改是有效的，别过了一会儿数据自己没了，不见了，那就好玩儿了。

3.2 事务隔离级别

总之，面试问你事务，先聊一下ACID，然后聊聊隔离级别

（1）读未提交，Read Uncommitted：这个很坑爹，就是说某个事务还没提交的时候，修改的数据，就让别的事务给读到了，这就恶心了，很容易导致出错的。这个也叫做脏读。

（2）读已提交，Read Committed（不可重复读）：这个比上面那个稍微好一点，但是一样比较尴尬

就是说事务A在跑的时候， 先查询了一个数据是值1，然后过了段时间，事务B把那个数据给修改了一下还提交了，此时事务A再次查询这个数据就成了值2了，这是读了人家事务提交的数据啊，所以是读已提交。

这个也叫做不可重复读，就是所谓的一个事务内对一个数据两次读，可能会读到不一样的值。如图：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-16-57-00.png width='100%'/></div><br/>

（3）可重复读，Read Repeatable：这个比上面那个再好点儿，就是说事务A在执行过程中，对某个数据的值，无论读多少次都是值1；哪怕这个过程中事务B修改了数据的值还提交了，但是事务A读到的还是自己事务开始时这个数据的值。如图：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-16-57-43.png width='100%'/></div><br/>

（4）幻读：不可重复读和可重复读都是针对两个事务同时对某条数据在修改，但是幻读针对的是插入

比如某个事务把所有行的某个字段都修改为了2，结果另外一个事务插入了一条数据，那个字段的值是1，然后就尴尬了。第一个事务会突然发现多出来一条数据，那个数据的字段是1。

那么幻读会带来啥问题呢？因为在此隔离级别下，例如：事务1要插入一条数据，我先查询一下有没有相同的数据，但是这时事务2添加了这条数据，这就会导致事务1插入失败，并且它就算再一次查询，也无法查询到与其插入相冲突的数据，同时自身死活都插入不了，这就不是尴尬，而是囧了。

（5）串行化：如果要解决幻读，就需要使用串行化级别的隔离级别，所有事务都串行起来，不允许多个事务并行操作。如图：

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-16-58-34.png width='100%'/></div><br/>

（6）MySQL的默认隔离级别是Read Repeatable，就是可重复读，就是说每个事务都会开启一个自己要操作的某个数据的快照，事务期间，读到的都是这个数据的快照罢了，对一个数据的多次读都是一样的。

接下来我们聊下MySQL是如何实现Read Repeatable的吧，因为一般我们都不修改这个隔离级别，但是你得清楚是怎么回事儿，MySQL是通过MVCC机制来实现的，就是多版本并发控制，multi-version concurrency control。

当我们使用innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。

事务id，在mysql内部是全局唯一递增的，事务id=1，事务id=2，事务id=3

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-16-59-34.png width='100%'/></div><br/>

事务id=121的事务，查询id=1的这一行的时候，一定会找到创建事务id <= 当前事务id的那一行

```select * from table where id=1;```就可以查到上面那一行

事务id=122的事务，将id=1的这一行给删除了，此时就会将id=1的行的删除事务id设置成122

事务id=121的事务，再次查询id=1的那一行，能查到吗？

能查到，要求创建事务id <= 当前事务id，当前事务id < 删除事务id

事务id=121的事务，查询id=2的那一行，查到name=李四

事务id=122的事务，将id=2的那一行的name修改成name=小李四

事务id=121的事务，查询id=2的那一行，答案是：李四，创建事务id <= 当前事务id，当前事务id < 删除事务id

在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；

同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。

那么如果某个事务执行期间，别的事务更新了一条数据呢？这个很关键的一个实现，其实就是在innodb中，是插入了一行记录，然后将新插入的记录的创建时间设置为新的事务的id，同时将这条记录之前的那个版本的删除时间设置为新的事务的id。

现在get到这个点了吧？这样的话，你的这个事务其实对某行记录的查询，始终都是查找的之前的那个快照，因为之前的那个快照的创建时间小于等于自己事务id，然后删除时间的事务id比自己事务id大，所以这个事务运行期间，会一直读取到这条数据的同一个版本。

记住，聊到事务隔离级别，必须把这套东西给喷出来，尤其是mvcc，说实话，市面上相当大比重的java程序员，对mvcc是不了解的

## 59、你能说说MySQL数据库锁的实现原理吗？如果死锁了咋办

1、面试题

数据库锁有哪些类型？

锁是如何实现的？

MySQL行级锁有哪两种？

一定会锁定指定的行么？为什么？

悲观锁和乐观锁是什么？使用场景是什么？

mysql死锁原理以及如何定位和解决？

2、面试官心里分析

说实话，聊mysql的话，我肯定也是循序渐进慢慢问的，先聊下存储引擎，然后问问索引一半怎么用，一半用innodb存储引擎，加上联合索引能玩儿好，明白什么是聚簇索引，再熟悉事务那套东西，包括spring的事务传播之类的，那么数据库常见的开发都没问题了。
接着就是聊聊锁，因为如果对锁没了解的话，线上系统其实进场有时候，在高并发访问下，会出现一些死锁的问题，或者是等待锁时间过长就超时了，偶尔会有这种问题的，所以会问问你锁的问题。

3、面试题剖析

（1）mysql锁

先跟面试官聊下，mysql的锁类型吧，一般其实就是表锁、行锁和页锁。

一般myisam会加表锁，就是myisam引擎下，执行查询的时候，会默认加个表共享锁，也就是表读锁，这个时候别人只能来查，不能写数据的；然后myisam写的时候，也会加个表独占锁，也就是表写锁，别人不能读也不能写。

这个myisam因为很少用了，所以别去管他了，面试的时候来这么一句就ok了。

所以话说回来，大家也发现了，myisam其实在实际生产中，我们曾经就是在报表系统里用的是最多的，当年es和kylin没出来的时候，大数据系统计算好的报表数据，都是放mysql的myisam里的，一般就是每天凌晨导入一批数据，那个时候别人不需要查询，没人凌晨来看报表；然后白天也没有写入，就是别人纯查询，建好索引，查询性能还是不错的，单表支撑千万级别数据没问题。

报表系统，有一次，一般来说hadoop计算完大批量的报表数据在凌晨就算完了，没有人看报表的；但是确实有一次是hadoop出了问题，是在上午11点还在计算往表里面大规模大批量的插入数据，当时造成了很严重的锁表，别人查就查不出来，我们的报表系统的用户在查看报表的时候，点504，点504，超时。

超大的case。。。。

这个页级锁，一般几乎很少用，你提一句就ok了，我们不多说了。

其实面试官重点还是跟你聊聊行锁就好了，是innodb引擎一般用行锁，但是也有表锁。
innodb的行锁有共享锁（S）和排他锁（X），两种，其实说白了呢，共享锁就是，多个事务都可以加共享锁读同一行数据，但是别的事务不能写这行数据；排他锁，就是就一个事务可以写这行数据，别的事务只能读，不能写。

innodb的表锁，分成意向共享锁，就是说加共享行锁的时候，必须先加这个共享表锁；还有一个意向排他锁，就是说，给某行加排他锁的时候，必须先给表加排他锁。这个表锁，是innodb引擎自动加的，不用你自己去加。

insert、update、delete，innodb会自动给那一行加行级排他锁

select，innodb啥锁都不加，因为innodb大家记得么，默认实现了可重复读，也就是mvcc机制，所以多个事务随便读一个数据，一般不会有冲突，大家就读自己那个快照就可以了，不涉及到什么锁的问题

但是innodb从来不会自己主动加个共享锁的，除非你用下面的语句自己手动加个锁：

手动加共享锁：```select * from table where id=1 lock in share mode```，那你就给那一行加了个共享锁，其他事务就不能来修改这行数据了

手动加排他锁：```select * from table where id=1 for update```，那你就给那一行加了个排他锁，意思就是你准备修改，别的事务就别修改了，别的事务的修改会hang住。这个要慎用，一般我们线上系统不用这个，容易搞出问题来。

所以看到这儿，我们琢磨琢磨默认的数据库锁机制，各位同学

对一行数据，如果有人在修改，会加个排他锁，然后你不能修改，你只能等着获取这把锁，但是这个时候你可以随便select，你就是查询你的事务开始之前那行数据的某个版本而已。然后如果你修改某行数据，会同时拿这个表的排他锁，但是呢，如果不同的事务修改不同的行，会拿不同行的行级排他锁，但是大家都会拿一个表的排他锁，ok，实际上innodb的表级排他锁可以随便拿，这个是没冲突的。

所以这个就是mysql innodb存储引擎的默认锁模式，其实还挺不错的。相当于就是一行数据，同一个时刻只能一个人在修改，但是别人修改，你可以随便读，读是读某个版本的，走mvcc机制。大家理解这个就好。

（2）悲观锁和乐观锁是啥？

mysql里的悲观锁是走```select * from table where id=1 for update```，就这个，意思是我很悲观，我担心自己拿不到这把锁，我必须先锁死，然后就我一个人可以干这事儿，别人都干不了了，不能加共享锁，也不能加排他锁。

乐观锁，就是说我觉得应该没啥问题，我修改的时候感觉差不多可以获取到锁，不需要提前搞一把锁，我就先查出来某个数据，```select id,name,version from table where id=1```，接着再执行各种业务逻辑之后再修改，```update table set name=’新值’,version=version+1 where id=1 and version=1```，就是说每次修改，比较一下这条数据的当前版本号跟我之前查出来的版本号是不是一样的，如果是一样的就修改然后把版本号加1，否则就不会更新任何一行数据，此时就重新查询后再次更新。

一般悲观锁什么时候用呢？比如你查出来了一条数据，要在内存中修改后再更新到数据库中去，但是如果这个过程中数据被别人更新了，你是不能直接干这个操作的，这个时候，你就得走上面那个操作，查询之后就不让别人更新了，你搞完了再说。

但是真有这种场景，推荐你还是用乐观锁把，悲观锁实现简单一点，但是太有风险了，很容易很容易死锁，比如事务A拿了数据1的锁，事务B拿了数据2的锁，然后事务A又要获取数据2的锁就会等待，事务B又要获取数据1的锁，也会等待，此时尴尬了，死锁，卡死，互相等待，永不释放。

所以select ... for update这个语法，轻易不要用，我们几乎线上很少用。

（3）死锁

事务A

```select * from table where id=1 for update```

事务B

```select * from table where id=2 for update```

事务A

```select * from table where id=2 for update```

事务B

```select * from table where id=1 for update```

常见的死锁就是类似上面那种，给大家说过了，分别都持有一个锁，结果还去请求别人持有的那把锁，结果就是谁也出不来，死锁了
情况太多，不一一列举了，其实就给大家说下发现死锁的时候怎么排查吧

其实很简单，就是找dba看一下死锁日志，就ok了，然后根据对应的sql，找下对应的代码，具体判断一下为啥死锁了

## 60、MySQL的SQL调优一般都有哪些手段？你们一般怎么做

1、面试题

SQL调优的常用手段

2、面试官心里分析

说实话，这个其实就是针对你有没有最最基础的线上SQL跑的慢的优化能力

3、面试题剖析

如果是应付面试，我们实在是不可能深入讲mysql的SQL优化，以后架构班里都会深入讲解，但是这里给大家说一句，互联网公司的系统，一般很少需要复杂的SQL优化

为啥呢？因为我说过很多次了，保持SQL简单，一般90%的SQL都建议是单表查询，join等逻辑放java代码里实现，不要放SQL里。

既然是单表查询了，你觉得还能有什么性能问题么？对吧

如果某个线上SQL跑的慢，十有八九就是因为那个SQL没有用索引，所以这个时候，第一步就是去看MySQL的执行计划，看看那个SQL有没有用到索引，如果没有，那么就改写一下SQL让他用上索引，或者是额外加个索引。

我的面试突击课里就讲这种互联网公司最经典和常用的SQL优化手段，其他的大家为了面试准备，可以临时去网上搜个帖子，MySQL SQL优化，随便记住一些到时候说说即可。

我这里其实主要就是讲下怎么看SQL的执行计划，这个是码农必备能力，必须能看懂执行计划，一般其实就是看SQL有没有走索引，你倒是可以在这个环节重点说下你对执行计划这块的理解就ok

```explain select * from table```，就ok了

```table | type | possible_keys | key | key_len | ref | rows | Extra```

+ table：哪个表
+ type：这个很重要，是说类型，all（全表扫描），const（读常量，最多一条记录匹配），eq_ref（走主键，一般就最多一条记录匹配），index（扫描全部索引），range（扫描部分索引）
+ possible_keys：显示可能使用的索引
+ key：实际使用的索引
+ key_len：使用索引的长度
+ ref：联合索引的哪一列被用了
+ rows：一共扫描和返回了多少行
+ extra：using filesort（需要额外进行排序），using temporary（mysql构建了临时表，比如排序的时候），using where（就是对索引扫出来的数据再次根据where来过滤出了结果）

## 61、聊聊Socket的工作原理？Socket跟TCP IP之间是啥关系？

1、面试题

说说socket通信的原理？

2、面试官心里分析

其实不知道大家发现没有，网络相关的问题，都是围绕着所谓的七层模型，或者是四层模型去走的。聊完四层模型，接着就是一次请求的全过程，紧接着就是聊传输层的tcp的连接，然后就是传输层的tcp协议之上的socket编程，接下来还会聊聊应用层的http协议。

所以说，来吧，这都是最最基础的网络知识。

3、面试题剖析

其实说白了，socket就是在传输层里把tcp/ip协议给封装了一下，我们程序员一般都是面向socket来编程的，比如java原生就支持socket网络编程的。

大体来说这个步骤，就是我们搞一个ServerSocket无限等待别人来连接你，然后某个机器要跟你连接，就在本地创建一个socket去连接你，然后建立连接之后，在服务器上，ServerSocket也会创建出来一个socket的。通过客户端的socket跟服务端的socket进行通信，我给你写数据，你读数据，你给我写数据，我读数据，就这个过程。

当然这个底层，比如建立连接和释放连接，都是基于tcp三次握手和四次挥手的规范来搞的，包括基于tcp协议传输数据，其实就跟我们之前说的一样，都是封装个tcp数据包，里面有tcp报头，整了端口号啥的，然后封装在ip数据包里，最后封在以太网数据包里传递。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-23-20-21-33.png width='100%'/></div><br/>

## 62、进程间是如何通信的？线程间又如何切换呢？

1、面试题

进程间是如何通信的？线程间又如何切换呢？

2、面试官心里分析

这个问题不是高频基础问题，但是确实可能有人会问，因为怎么说呢，计算机基础，就这点儿东西，网络、cpu、磁盘、内存、进程，所以可能有人会看看你的基础知识咋样，所以问问你这个问题。

3、面试题剖析

进程间的通信有很多种方式，比如说：管道（pipe）、命名管道（fifo）、消息队列，共享内存（System V）

（1）管道（pipe）

unix操作系统里面，有一个fork操作，可以创建进程的子进程，或者说是复制一个进程完全一样的子进程，共享代码空间，但是各自有独立的数据空间，不过子进程的数据空间是拷贝父进程的数据空间的。

管道机制要求的是两个进程之间是有血缘关系的，就比如fork出来的父子进程。

linux操作系统里，管道用来缓存要在进程间传输的数据，管道是一个固定大小的缓冲区，是4kb。管道中的数据一旦被读取出来，就不在管道里了。

但是如果管道满了，那么写管道的操作就阻塞了，直到别人读了管道的数据；反之如果管道是空的，那么读操作就阻塞了。就这个意思。管道一边连着一个进程的输出，一边连着一个进程的输入，然后就一个进程写数据，另外一个进程读数据，两个进程都没了，管道也就没了。管道是半双工的，就是数据只能流向一个方向，比如说你架设一个管道，只能一个进程写，另外一个进程读。

linux里面对管道的实现，是用了两个文件，指向了一个VFS（虚拟文件系统）的索引节点inode，然后VFS索引节点指向一个物理页面，接着一个进程通过自己关联的那个文件写数据，另外一个进程通过自己关联的那个文件读数据。

（2）命名管道（fifo）

管道的通信，要求必须是父子关系的进程间通信，就受到了限制，所以可以用命名管理来解决这个问题。

之前的管道，是没有名字的，所以必须是有父子关系的进程才能使用。但是这个命名管道是有名字的。这个命名管道，相当于是一个有名字的文件，是有路径的，所以没有血缘关系的进程多可以通过这个命名管道来通信，名字在文件系统上，数据在内存里。其他的跟管道一样，一个进程写，一个进程读，也是半双工的，数据只能单向流动。

（3）消息队列

linux的消息队列可以认为是个链表结构，linux内核有一个msgque链表，这个链表里每个指针指向一个msgid_ds结构，这个结构就描述了一个消息队列。然后进程之间就通过这个消息队列通信就可以，一样是写入数据和消费数据。消息队列的好处就是对每个消息可以指定类型，消费的时候就消费指定类型的消息就行了，功能更多一些。这种方式其实用的不多的。

（4）共享内存

一块物理内存被映射到两个进程的进程地址空间，所以进程之间互相都可以立即看到对方在共享内存里做出的修改，但是因为是共享内存，所以需要锁来保证同步。这个说对了很复杂，我在这里就不多说了，我觉得如果被人问到这个问题，短期内突击的话，回答到这个程度就行了，就是知道有哪些方式。如果你要深入理解各种机制，那是要好好学习linux的各种东西了。

（5）线程间如何切换

一个进程的多个线程间切换的时候就涉及到了上下文切换，这个东西说复杂了就很复杂，但是简单来说，就是有一个时间片算法，cpu给每个线程一个时间片来执行，时间片结束之后，就保存这个线程的状态，然后切换到下一个线程去执行，这就是所谓多线程并发执行的原理，就是多个线程来回来去切换，每个线程就一个时间片里执行。太复杂的我也不讲了，大家就记住一个线程上下文切换指的是什么就行了。

## 63-64、你能聊聊BIO、NIO、AIO分别都是啥？有什么区别？（上 下）

1、面试题

nio、bio、aio都是什么以及有什么区别？说说nio的原理？

2、面试官心里分析

如果聊io这块，我就必问这个问题，因为io的那些过于基础的知识，各种流的使用，是用来考察应届生和培训班刚出来的同学的，正常问一个有经验的开发人员，io这块就是聊聊几种io模式，以及同步、异步、阻塞和非阻塞几种io的概念。

3、面试题剖析

3.1 BIO

这个其实就是最传统的网络通信模型，就是BIO，同步阻塞式IO，简单来说大家如果参加过几个月的培训班儿应该都知道这种BIO网络通信方式。就是服务端创建一个ServerSocket，然后客户端用一个Socket去连接那个ServerSocket，然后ServerSocket接收到一个Socket的连接请求就创建一个Socket和一个线程去跟那个Socket进行通信。

然后客户端和服务端的socket，就进行同步阻塞式的通信，客户端socket发送一个请求，服务端socket进行处理后返回响应，响应必须是等处理完后才会返回，在这之前啥事儿也干不了，这可不就是同步么。

这种方式最大的坑在于，每次一个客户端接入，都是要在服务端创建一个线程来服务这个客户端的，这会导致大量的客户端的时候，服务端的线程数量可能达到几千甚至几万，几十万，这会导致服务器端程序的负载过高，最后崩溃死掉。

要么你就是搞一个线程池，固定线程数量来处理请求，但是高并发请求的时候，还是可能会导致各种排队和延时，因为没那么多线程来处理。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-24-22-20-35.png width='100%'/></div><br/>

3.2 NIO

JDK 1.4中引入了NIO，这是一种同步非阻塞的IO，基于Reactor模型。

NIO中有一些概念：

比如Buffer，缓冲区的概念，一般都是将数据写入Buffer中，然后从Buffer中读取数据，有IntBuffer、LongBuffer、CharBuffer等很多种针对基础数据类型的Buffer。

还有Channel，NIO中都是通过Channel来进行数据读写的。

包括Selector，这是多路复用器，selector会不断轮询注册的channel，如果某个channel上发生了读写事件，selector就会将这些channel获取出来，我们通过SelectionKey获取有读写事件的channel，就可以进行IO操作。一个Selector就通过一个线程，就可以轮询成千上万的channel，这就意味着你的服务端可以接入成千上万的客户端。

这块其实相当于就是一个线程处理大量的客户端的请求，通过一个线程轮询大量的channel，每次就获取一批有事件的channel，然后对每个请求启动一个线程处理即可。

这里的核心就是非阻塞，就那个selector一个线程就可以不停轮询channel，所有客户端请求都不会阻塞，直接就会进来，大不了就是等待一下排着队而已。

这里的核心就是因为，一个客户端不是时时刻刻都要发送请求的，没必要死耗着一个线程不放吧，所以NIO的优化思想就是一个请求一个线程。只有某个客户端发送了一个请求的时候，才会启动一个线程来处理。

所以为啥是非阻塞呢？因为无论多少客户端都可以接入服务端，客户端接入并不会耗费一个线程，只会创建一个连接然后注册到selector上去罢了，一个selector线程不断的轮询所有的socket连接，发现有事件了就通知你，然后你就启动一个线程处理一个请求即可，但是这个处理的过程中，你还是要先读取数据，处理，再返回的，这是个同步的过程。

所以NIO是同步非阻塞的。

工作线程，从channel里读数据，是同步的，是工作线程自己去干这个事儿，卡在那儿，专门干读数据的这个活儿，数据没读完，你就卡死在这儿了；然后往channel里写数据，也是你自己去干这个事儿，卡死在这儿了，数据没写完，你就卡在这儿了。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-24-22-27-21.png width='100%'/></div><br/>

3.3 AIO

AIO是基于Proactor模型的，就是异步非阻塞模型。

每个连接发送过来的请求，都会绑定一个buffer，然后通知操作系统去异步完成读，此时你的程序是会去干别的事儿的，等操作系统完成数据读取之后，就会回调你的接口，给你操作系统异步读完的数据。

然后你对这个数据处理一下，接着将结果往回写。

写的时候也是给操作系统一个buffer，让操作系统自己获取数据去完成写操作，写完以后再回来通知你。

工作线程，读取数据的时候，是说，你提供给操作系统一个buffer，空的，然后你就可以干别的事儿了，你就把读数据的事儿，交给操作系统去干，操作系统内核，读数据将数据放入buffer中，完事儿了，来回调你的一个接口，告诉你说，ok，buffer交给你了，这个数据我给你读好了

写数据的时候也是一样的的，把放了数据的buffer交给操作系统的内核去处理，你就可以去干别的事儿了，操作系统完成了数据的写之后，级会来回调你，告诉你说，ok，哥儿们，你交给我的数据，我都给你写回到客户端去了

3.4 同步阻塞、同步非阻塞、异步非阻塞

但是这里为啥叫BIO是同步阻塞呢？这个其实不是针对网络编程模型来说的，而是针对文件IO操作来说的，因为用BIO的流读写文件，是说你发起个IO请求直接卡死，等待，必须等着搞完了这次IO才能返回。

BIO的这个同步阻塞，不是完全针对的网络通信模型去说的，针对的是磁盘文件的IO读写，FileInputStream，BIO，卡在那儿，直到你读写完成了才可以。

NIO为啥是同步非阻塞？就是说通过NIO的FileChannel发起个文件IO操作，其实发起之后就返回了，你可以干别的事儿，这就是非阻塞，但是接下来你还得不断的去轮询操作系统，看IO操作完事儿了没有。

你也可以使用FileChannel这种NIO的模型，去读写磁盘文件，读数据，发起读数据的请求之后，你不是阻塞住的，你可以干别的事儿，但是你在干别的事儿的同时，还得来时不时的自己去轮询操作系统读数据的状态，看看人家读好了没有。

AIO为啥是异步非阻塞？就是说通过AIO发起个文件IO操作之后，你立马就返回可以干别的事儿了，接下来你也不用管了，操作系统自己干完了IO之后，告诉你说ok了。同步就是自己还得主动去轮询操作系统，异步就是操作系统反过来通知你。

你也可以基于AIO的文件读写的api去读写磁盘文件，你发起一个文件读写的操作之后，交给操作系统，你就不去管他了，直到操作系统自己完成之后，会来回调你的一个接口，通知你说，ok，这个数据读好了，那个数据写完了

3.5 BIO、NIO、AIO的demo代码

上网找。

## 65、线上服务器CPU 100%了！该怎么排查、定位和解决？

1、面试题

线上服务器的cpu使用达到100%了，如何排查、定位和解决该问题？

2、面试官心里分析

说实话，这个问题是面试的时候，聊基础，最常问的一个问题，就是看看你有没有处理过高负载的线上问题场景。所以很多大公司考察你的基本功，肯定会问这个。其实这个你干过就是干过，掌握就是掌握，只要干过，所有人都是一样的步骤，没区别。

3、面试题剖析

其实核心思路，就是找到这台服务器上，是哪个进程的哪个线程的哪段代码，导致cpu 100了，主要就是考察你是否熟练运用一些线上的命令。

这里我可以给大家说一个我们线上的经验，就是之前有一个bug，是一个很年轻的同学写的，就是我们当时是定了异常日志是写到es里去的

```java
public void log(String message) {

try {

// 往es去写

} catch(Exception e) {

// 一旦出现异常，又调用了自己本身，递归死循环。所以出现 cpu 100%，卡死了
log(message);

}

}

```

线上事故，es集群出了点问题，没法写，最后出现线上几十台机器，全部因为这一行代码，全体cpu 100%，卡死了.

（1）定位耗费cpu的进程

top -c，就可以显示进程列表，然后输入P，按照cpu使用率排序，你会看到类似下面的东西

```shell
PID         USER       PR   NI    VIRT         RES  SHR          S      %CPU      %MEM    TIME+ COMMAND

43987       root       20   0     28.2g       4.5g  68m S    99.0        24.0    44333.4  java -Xms...

```

大概类似上面这样，能看到哪个进程，CPU负载最高，还有启动这个进程的命令，比如一般就是java啥啥的。

（2）定位耗费cpu的线程

top -Hp 43987，就是输入那个进程id就好了，然后输入P，按照cpu使用率排序，你会看到类似下面的东西

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-24-23-01-47.png width='100%'/></div><br/>

大概类似上面那样，你就可以看到这个进程里的哪个线程耗费cpu最高

（3）定位哪段代码导致的cpu过高

printf “%x\n” 16872，把线程pid转换成16进制，比如41e8

```jstack 43987 | grep ‘0x41e8’ -C5 --color```

这个就是用jstack打印进程的堆栈信息，而且通过grep那个线程的16进制的pid，找到那个线程相关的东西，这个时候就可以在打印出的代码里，看到是哪个类的哪个方法导致的这个cpu 100%的问题。

## 66、线上机器的一个进程用kill命令杀不死该怎么办？磁盘空间快满了又该怎么处理？

1、面试题

线上进程kill不掉怎么办

2、面试官心里分析

但是可能就是想考察一下你有没有处理过类似的问题

3、面试题剖析

我们公司有一套自己研发的发布系统，你每次部署，都是走发布系统，告诉他一个git仓库的地址，那个系统会自动从git仓库拉取代码，基于maven来打包，你还可以指定你要用的profile，maven打包的时候会用对应的profile打对应环境的包，打完jar包之后，就会java -jar之类的来启动。

当时那个发布系统，他自己在每台机器上有一个进程，发布和启动的时候，他启动的那个进程，不是直接java -jar来启动的，发布系统的一个进程搞了一个子进程，子进程是我们的系统进程。

这个其实就是线上可能遇到的一个问题，我们之前确实就是遇到过这个问题，kill一个进程死活杀不死，那个进程成了僵尸进程，就是zombie状态。这是因为这个进程释放了资源，但是没有得到父进程的确认。

```ps aux```，看看STAT那一栏，如果是Z，那么就是zombie状态的僵尸进程

```ps -ef | grep 僵尸进程id```，可以找到父进程id

然后先kill掉父进程即可

1、面试题

服务器存储空间快满了（95%），还有一个小时存储就满了，在不影响服务正常运行的情况下，该如何解决？

2、面试官心里分析

这个确实没什么好说的，无非就是用一些一些线上的场景和问题来考考你平时一般怎么处理的，线上机器磁盘满，一般啥原因，不就是日志太多了给写满了么。。。对吧，我们不说别的，就说说这最基本的就行了

3、面试题剖析

```df -h```，先看看磁盘使用的情况

然后就是到你的系统部署的地方，一般就是tomcat下的日志、spring boot的日志，去看看，如果过多，就删除掉一些日志就行了，自己注意让tomcat或者nginx之类的日志输出，按天切割，这样你还可以写个shell脚本，crontab定时，定期删除7天以前的日志

要是不行，那就：```find / -size +100M |xargs ls -lh```，找找大于100m的文件，但是如果有大量的小文件，那么这样是不行的

或者是用：```du -h >fs_du.log```，看看各个目录占用的磁盘空间大小，看看是不是哪个目录有大量的小文件

其实面试官无非就是看看是不是知道常见的命令罢了，如果不是。那那个面试官就得再提示多一些细节，到底要考察你什么。但是简单问一个磁盘占用排查，就是常见这几个命令罢了。

## 67、再谈原子性：Java规范规定所有变量写操作都是原子的

这个java并发技术底层的原理，volatile、synchronized的对可见性、有序性的保障的语义，底层其实是基于内存屏障来实现的，内存屏障又是通过硬件来实现的，硬件底层原理（高速缓存、写缓冲器、无效队列），各种内存屏障在底层硬件层面他的实现的原理有到底是什么？

回过头来看看，volatile和synchronized通过各种内存屏障的使用，底层在硬件级别的实现原理到底是什么

可见性、有序性、原子性，都彻底通透了以后，借着硬件级别的原理，给大家再说一下CAS底层的硬件级别的原理

**原子性--内容：**

Applications apps;

线程1：

apps = loadedApps; // 原子的，不需要AtomicReference来处理

java语言规范里面，int i = 0，resource = loadedResoures，flag = true，各种变量的简单的赋值操作，规定都是原子的

包括引用类型的变量的赋值写操作，也是原子的

你赋值的时候，要保证没有人先赋值过，没有人修改过，你才能赋值，AtomicReference的CAS操作来实现的，之前给大家讲解过的

但是很多复杂的一些操作，i++，先读取i的值，再跟新i的值，i = y + 2，先读取y的值，再更新i的值，这种复杂操作，不是简单赋值写，他是有计算的过程在里面的，此时java语言规范默认是不保证原子性的

volatile，保证的可见性和有序性，不保证原子性，杠精，偷换概念，胡说八道；i++，i = y + 2，不是volatile可以保证原子性的

## 68、32位Java虚拟机中的long和double变量写操作为何不是原子的？

原子性这块，特例，32位虚拟机里的long/double类型的变量的简单赋值写操作，不是原子的，long i = 30，double c = 45.0，在32位虚拟机里就不是原子的，因为long和double是64位的

0000 0000 0000 0000 0000 0000 0000 0000  0000 0000 0000 0000 0000 0000 0000 0000

如果多个线程同时并发的执行long i = 30，long是64位的，就会导致有的线程在修改i的高32位，有的线程在修改i的低32位，多线程并发给long类型的变量进行赋值操作，在32位的虚拟机下，是有问题的

就可能会导致多线程给long i = 30赋值之后，导致i的值不是30，可能是-3333344429，乱码一样的数字，就是因为高低32位赋值错了，就导致二进制数字转换为十进制之后是一个很奇怪的数字

## 69、volatile原来还可以保证long和double变量写操作的原子性

volatile对原子性保障的语义，在java里很有限的，几乎可以忽略不计。32位的java虚拟机里面，对long/double变量的赋值写是不原子的，此时如果对变量加上了volatile，就可以保证在32位java虚拟机里面，对long/double变量的赋值写是原子的了

int i = 0，原子性，volatile，java语言规范就规定了，原子性的

volatile long i;

多个线程执行：i = 30，此时就不要紧了，因为volatile修饰了，就可以保证这个赋值操作是原子的了

你以后出去面试也可能会遇到杠精面试官，你要说volatile是保证可见性和有序性的，不保证原子性，杠精面试官，素质差，很二，心胸很狭隘，volatile可以保证原子性，此时看过这一讲之后

i++，复杂的一些场景

```java
resources = loadResources();
resources.execute();
ready = true;
```

## 70、到底有哪些操作在Java规范中是不保证原子性的呢？

所有变量的简单赋值写操作，java语言规范原生给你保证原子性的；32位java虚拟机里的long/double是不保证赋值写的原子性的；volatile可以解决这个问题；不保证原子性的一些操作呢？

```java
i++

i = y + 1

i = x * y ==> 先把x和y分别从主内存里加载到工作内存里面来，
然后再从工作内存里加载出来执行计算（处理器），计算后的结果写回到工作内存里去，
最后还要从工作内存里把i的最新的值刷回主内存 
```

CAS，AtomicInteger => compareAndSet

你敢说他是原子的？ 不能

```java
volatile x = 1;
volatile y = 2;
volatile i = x * y;
```

我之前给大家已经说过了，画图都演示过了

```java
FSDirectory dir = new FSDirectory();

synchronized(dir) {
// 加锁 来解决原子性  
dir.add();
dir.remove();
dir.insert();
}
```

## 71-72、可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器（上 下）

1，从硬件的级别来考虑一下可见性的问题

+ 可见性发生的硬件级别场景一：

每个处理器都有自己的寄存器（register），所以多个处理器各自运行一个线程的时候，可能导致某个变量给放到寄存器里去，接着就会导致各个线程没法看到其他处理器寄存器里的变量的值修改了。

在寄存器的级别，导致变量副本的更新，无法让其他处理器看到。

+ 可见性发生的硬件级别场景二：

然后一个处理器运行的线程对变量的写操作都是针对写缓冲来的（store buffer）并不是直接更新主内存，所以很可能导致一个线程更新了变量，但是仅仅是在写缓冲区里罢了，没有更新到高速缓存或者主内存里去。

这个时候，其他处理器的线程是没法读到它的写缓冲区的变量值的，所以此时就是会有可见性的问题。

+ 可见性发生的硬件级别场景三：

然后即使这个时候一个处理器的线程更新了写缓冲区之后，将更新同步到了自己的高速缓存里（cache，或者是主内存），然后还把这个更新通知给了其他的处理器，但是其他处理器可能就是把这个更新值放到无效队列里去，没有更新他的高速缓存。

此时其他处理器的线程从高速缓存里读数据的时候，读到的还是过时的旧值。

2 ，那如何实现可见性呢？

如果要实现可见性的话，其中一个方法就是通过MESI协议（一种缓存协议），这个MESI协议实际上有很多种不同的时间，因为他不过就是一个协议罢了，具体的实现机制要靠具体底层的系统如何实现

根据具体底层硬件的不同，MESI协议的实现是有区别的

比如说MESI协议有一种实现，就是一个处理器将另外一个处理器的高速缓存中的更新后的数据拿到自己的高速缓存中来更新一下，这样大家的缓存不就实现同步了，然后各个处理器的线程看到的数据就一样了

为了实现MESI协议，有两个配套的专业机制要给大家说一下：***flush处理器缓存、refresh处理器缓存。***

flush处理器缓存，他的意思就是把自己更新的值刷新到高速缓存里去（或者是主内存），因为必须要刷到高速缓存（或者是主内存）里，才有可能在后续通过一些特殊的机制让其他的处理器从自己的高速缓存（或者是主内存）里读取到更新的值

除了flush以外，他还会发送一个消息到总线（bus），通知其他处理器，某个变量的值被他给修改了

refresh处理器缓存，他的意思就是说，处理器中的线程在读取一个变量的值的时候，如果发现其他处理器的线程更新了变量的值，必须从其他处理器的高速缓存（或者是主内存）里，读取这个最新的值，更新到自己的高速缓存中

所以说，为了保证可见性，**在底层是通过MESI协议、flush处理器缓存和refresh处理器缓存，这一整套机制来保障的**

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-25-13-01-57.png width='100%'/></div><br/>

>要记住，flush和refresh，这两个操作，flush是强制刷新数据到高速缓存（主内存），不要仅仅停留在写缓冲器里面；refresh，是从总线嗅探发现某个变量被修改，必须强制从其他处理器的高速缓存（或者主内存）加载变量的最新值到自己的高速缓存里去.

内存屏障的使用，在底层硬件级别的原理，其实就是在执行flush和refresh，MESI协议是如何与内存屏障搭配使用的（flush、refresh）

volatile boolean isRunning = true;

isRunning = false; => 写volatile变量，就会通过执行一个内存屏障，在底层会触发flush处理器缓存的操作；while(isRunning) {}，读volatile变量，也会通过执行一个内存屏障，在底层触发refresh操作

之前给大家讲过那个volatile关键字的作用，对一个变量加了volatile修饰之后，对这个变量的写操作，会执行flush处理器缓存，把数据刷到高速缓存（或者是主内存）中，然后对这个变量的读操作，会执行refresh处理器缓存，从其他处理器的高速缓存（或者是主内存）中，读取最新的值

当然跟我们之前说的有一点点不一样，因为之前说的是写volatile变量的时候，一个是强制刷主内存，一个是过期掉其他处理器的高速缓存中的数据；读volatile变量的时候，会发现高速缓存中的值过期，然后强制从主内存加载最新值

其实这个东西吧，你没发现么，效果是一样的，他其实本质都是让一个线程写了volatie变量之后，另外一个变量立马可以读到volatile变量的值，只不过MESI协议的底层具体实现，根据cpu等硬件的不同，有多种不同的实现方式罢了。

## 73、深入探秘有序性：Java程序运行过程中发生指令重排的几个地方

我们写好的代码在实际执行的时候那个顺序可能在很多环节都会被人给重排序，一旦重排序之后，在多线程并发的场景下，就有可能会出现一些问题

（1）自己写的源代码中的执行顺序：这个是我们自己写的代码，一般来说就是按照我们自己脑子里想的样子来写

（2）编译后的代码的执行顺序：java里有两种编译器，一个是静态编译器（javac），一个是动态编译器（JIT）。javac负责把.java文件中的源代码编译为.class文件中的字节码，这个一般是程序写好之后进行编译的。JIT负责把.class文件中的字节码动态编译为JVM所在操作系统支持的机器码，一般在程序运行过程中进行编译。

在这个编译的过程中，编译器是很有可能调整代码的执行顺序的，为了提高代码的执行效率，很可能会调整代码的执行顺序，JIT编译器对指令重排的还是挺多的

（3）处理器的执行顺序：哪怕你给处理器一个代码的执行顺序，但是处理器还是可能会重排代码，更换一种执行顺序，JIT编译好的指令的时候，还是可能会调整顺序

（4）内存重排序：有可能你这个处理器在实际执行指令的过程中，在高速缓存和写缓冲器、无效队列等等，硬件层面的组件，也可能会导致你的指令的执行看起来的顺序跟想象的不太一样

上述就是在我们写好java代码之后，从编译到执行的过程中，可能代码的执行顺序可能会有指令重排的地方，只要有指令重排就有一定可能造成程序执行异常

但是编译器和处理器不是胡乱的重排序的，他们会遵循一个关键的规则，就是数据依赖规则，如果说一个变量的结果依赖于之前的代码执行结果，那么就不能随意进行重排序，要遵循数据的依赖

比如说：

```java
int a = 3;
int b = 5;
int c = a * b;
```

那第三行代码依赖于上面两行代码，第一行和第二行代码可以重排序，但是第三行代码必须放在最下面

此外，之前给大家介绍过happens-before原则，就是有一些基本的规则是要遵守的，不会让你胡乱的重排序

在遵守一定的规则的前提下，有好几个层面的代码和指令都可能出现重排序

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-25-13-24-05.png width='100%'/></div><br/>

## 74、JIT编译器对创建对象的指令重排以及double check单例实践

JIT动态编译的时候，有可能会造成一个非常经典的指令重排

```java
public class MyObject {

private Resource resource;

public MyObject() {
this.resource = loadResource(); // 从配置文件里加载数据构造Resource对象
}

public void execute() {
this.resource.execute();
}

}

// 线程1:
MyObject myObj = new MyObject(); => 这个是我们自己写的一行代码

// 线程2：
myObj.execute();

// 步骤1：以MyObject类作为原型，给他的对象实例分配一块内存空间，objRef就是指向了分配好的内存空间的地址的引用，指针
objRef = allocate(MyObject.class);

// 步骤2：就是针对分配好内存空间的一个对象实例，执行他的构造函数，对这个对象实例进行初始化的操作，执行我们自己写的构造函数里的一些代码，对各个实例变量赋值，初始化的逻辑
invokeConstructor(objRef);

// 步骤3：上两个步骤搞定之后，一个对象实例就搞定了，此时就是把objRef指针指向的内存地址，赋值给我们自己的引用类型的变量，myObj就可以作为一个类似指针的概念指向了MyObject对象实例的内存地址
myObj = objRef;

```

有可能JIT动态编译为了加速程序的执行速度，因为步骤2是在初始化一个对象实例，这个步骤是有可能很耗时的，比如说你可能会在里面执行一些网络的通信，磁盘文件的读写，都有可能

JIT动态编译，指令重排，为了加速程序的执行性能和效率，可能会重排为，步骤1 -> 步骤3 -> 步骤2

线程1，刚刚执行完了步骤1和步骤3，步骤2还没执行，此时myObj已经不是null了，但是MyObject对象实例内部的resource是null

线程2，直接调用myObj.execute()方法， 此时内部会调用resource.execute()方法，但是此时resource是null，直接导致空指针

double check单例模式里面，就是可能会出现这样的JIT指令重排，如果你不加volatile关键字，会导致一些问题的发生，volatile是避免说步骤1、步骤3、步骤2，必须全部执行完毕了，此时才能试用myObj对象实例

## 75、现代处理器为了提升性能的指令乱序和猜测执行的机制！

指令乱序机制

指令不一定说是拿到了一个指令立马可以执行的，比如有的指令是要进行网络通信、磁盘读写，获取锁，很多种，有的指令不是立马就绪可以执行的，为了提升效率，在现代处理器里面都是走的指令的乱序执行机制

把编译好的指令一条一条读取到处理器里，但是哪个指令先就绪可以执行，就先执行，不是按照代码顺序来的。每个指令的结果放到一个重排序处理器中，重排序处理器把各个指令的结果按照代码顺序应用到主内存或者写缓冲器里

这就导致处理器可能压根儿就是乱序在执行我们代码编译后的指令

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-25-13-48-36.png width='100%'/></div><br/>

另外还有一个猜测执行，比如说if判断中有一坨代码，很可能先去执行if里的代码算出来结果，然后最后再来判断if是否成立

```java
int sum = 0

if(flag) {
for(int i = 0; i < 10; i++) {

}
}
```

## 76、高速缓存和写缓冲器的内存重排序造成的视觉假象

处理器会将数据写入写缓冲器，这个过程是store；从高速缓存里读数据，这个过程是load。写缓冲器和高速缓存执行load和store的过程，都是按照处理器指示的顺序来的，处理器的重排处理器也是按照程序顺序来load和store的

但是有个问题，就是在其他的处理器看到的一个视觉假象而言，有可能会出现看到的load和store是重排序的，也就是内存重排序

处理器的乱序执行和推测执行，都是指令重排序，这次说的是内存重排序，因为都是发生在内存层面的写缓冲器和高速缓存中的



这个内存重排序，有4种可能性：

（1）LoadLoad重排序：一个处理器先执行一个L1读操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再L1

（2）StoreStore重排序：一个处理器先执行一个W1写操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再W1

（3）LoadStore重排序：一个处理器先执行一个L1读操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再L1

（3）StoreLoad重排序：一个处理器先执行一个W1写操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再W1

给大家举个例子，比如说写缓冲器为了提升性能，有可能先后到来W1和W2操作了之后，他先执行了W2操作，再执行了W1操作。那这个时候其他处理器看到的可不就是先W2再W1了，这就是StoreStore重排序

共享变量：

```java
Resource resource = null;
Boolean resourceLoaded = false;

处理器0：

resource = loadResoureFromDisk();
resourceLoaded = true;
```

处理器1：

```java
while(!resourceLoaded) {
try {
Thread.sleep(1000);
} catch(Exception) {

}
}
resource.execute();
```

类似上面的代码，很可能处理器0先写了resource，再写了resourceLoaded。结果呢，写缓冲器进行了内存重排序，先落地了resourceLoaded = true了，此时resource还是null。此时处理器1就会看到resourceLoaded = true，就会对resource对象执行execute()方法，此时就会有空指针异常的问题

反正类似的情况，高速缓存和写缓冲器都可以自己对Load和Store操作的结果落地到内存进行各种不同的重排序，进而造成上述4种内存重排序问题的发生。

## 76、高速缓存和写缓冲器的内存重排序造成的视觉假象

处理器会将数据写入写缓冲器，这个过程是store；从高速缓存里读数据，这个过程是load。写缓冲器和高速缓存执行load和store的过程，都是按照处理器指示的顺序来的，处理器的重排处理器也是按照程序顺序来load和store的

但是有个问题，就是在其他的处理器看到的一个视觉假象而言，有可能会出现看到的load和store是重排序的，也就是内存重排序

处理器的乱序执行和推测执行，都是指令重排序，这次说的是内存重排序，因为都是发生在内存层面的写缓冲器和高速缓存中的

这个内存重排序，有4种可能性：

（1）LoadLoad重排序：一个处理器先执行一个L1读操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再L1

（2）StoreStore重排序：一个处理器先执行一个W1写操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再W1

（3）LoadStore重排序：一个处理器先执行一个L1读操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再L1

（3）StoreLoad重排序：一个处理器先执行一个W1写操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再W1

给大家举个例子，比如说写缓冲器为了提升性能，有可能先后到来W1和W2操作了之后，他先执行了W2操作，再执行了W1操作。那这个时候其他处理器看到的可不就是先W2再W1了，这就是StoreStore重排序

```java
//共享变量：

Resource resource = null;
Boolean resourceLoaded = false;

//处理器0：

resource = loadResoureFromDisk();
resourceLoaded = true;

//处理器1：

while(!resourceLoaded) {
try {
Thread.sleep(1000);
} catch(Exception) {

}
}
resource.execute();
```

类似上面的代码，很可能处理器0先写了resource，再写了resourceLoaded。结果呢，写缓冲器进行了内存重排序，先落地了resourceLoaded = true了，此时resource还是null。此时处理器1就会看到resourceLoaded = true，就会对resource对象执行execute()方法，此时就会有空指针异常的问题。

<div align='center'><img src=./images/java-基础01/java-基础01_2021-02-25-14-18-16.png width='100%'/></div><br/>

反正类似的情况，高速缓存和写缓冲器都可以自己对Load和Store操作的结果落地到内存进行各种不同的重排序，进而造成上述4种内存重排序问题的发生

## 77、synchronized锁同时对原子性、可见性以及有序性的保证

原子性、可见性、有序性，三块东西，都重新从比较细节和底层的层面，都在硬件的级别去给大家说了一下，到底是怎么回事，为什么会发生这个问题，从底层的层面来说了一下，以及大体上有没有什么办法可以来解决这些问题

原子性，基本的赋值写操作都是可以保证原子性的，复杂的操作是无法保证原子性的
可见性，MESI协议、flush、refresh，配合起来，才可以解决可见性
有序性，三个层次，最后一个层次有4种重排（LoadLoad、StoreStore、LoadStore、StoreLoad）

synchronized关键字，同时可以保证原子性、可见性以及有序性的

原子性的层面而言，他加了以后，有一个加锁和释放锁的机制，加锁了之后，同一段代码就只有他可以执行了

可见性，可以保证可见性的，他会通过加入一些内存屏障，他在同步代码块对变量做的写操作，都会在释放锁的时候，全部强制执行flush操作，在进入同步代码块的时候，对变量的读操作，全部会强制执行refresh的操作

更新的数据，别的县城关只要进入代码块，就一定可以读到的

有序性，synchronized关键字，他会通过加各种各样的内存屏障，来保证说，解决LoadLoad、StoreStore等等重排序

## 78、深入分析synchronized是如何通过加锁保证原子性的？


<div align='center'><img src=./images/java-基础01/java-基础01_2021-03-04-14-14-42.png width='100%'/></div><br/>

之前给大家简单说过synchronized加锁的原理，说白了，就是在进入加锁代码块的时候加一个monitorenter的指令，然后针对锁对象关联的monitor累加加锁计数器，同时标识自己这个线程加了锁

通过monitor里的加锁计数器可以实现可重入的加锁

在出锁代码块的时候，加一个monitorexit的指令，然后递减锁计数器，如果锁计数为0，就会标志当前线程不持有锁，释放锁

然后wait和notify关键字的实现也是依托于monitor实现的，有线程执行wait之后，自己会加入一个waitset中等待唤醒获取锁，notifyall操作会从monitor的waitset中唤醒所有的线程，让他们竞争获取锁

这边来深入分析一下那个加锁的底层原理

MyObject lock = new MyObject();

synchronized(lock) {

}

<div align='center'><img src=./images/java-基础01/java-基础01_2021-03-04-14-21-44.png width='100%'/></div><br/>

java对象都是分为对象头和实例变量两块的，其中实例变量就是大家平时看到的对象里的那些变量数据。然后对象头包含了两块东西，一个是Mark Word（包含hashCode、锁数据、GC数据，等等），另一个是Class Metadata Address（包含了指向类的元数据的指针）

在Mark Word里就有一个指针，是指向了这个对象实例关联的monitor的地址，这个monitor是c++实现的，不是java实现的。这个monitor实际上是c++实现的一个ObjectMonitor对象，里面包含了一个_owner指针，指向了持有锁的线程。

ObjectMonitor里还有一个entrylist，想要加锁的线程全部先进入这个entrylist等待获取机会尝试加锁，实际有机会加锁的线程，就会设置_owner指针指向自己，然后对_count计数器累加1次

各个线程尝试竞争进行加锁，此时竞争加锁是在JDK 1.6以后优化成了基于CAS来进行加锁，理解为跟之前的Lock API的加锁机制是类似的，CAS操作，操作_count计数器，比如说将_count值尝试从0变为1

如果成功了，那么加锁成功了；如果失败了，那么加锁失败了

然后释放锁的时候，先是对_count计数器递减1，如果为0了就会设置_owner为null，不再指向自己，代表自己彻底释放锁

如果获取锁的线程执行wait，就会将计数器递减，同时_owner设置为null，然后自己进入waitset中等待唤醒，别人获取了锁执行notify的时候就会唤醒waitset中的线程竞争尝试获取锁

有人会问，那尝试加锁这个过程，也就是对_count计数器累加操作，是怎么执行的？如何保证多线程并发的原子性呢？很简单，JDk 1.6之后，对synchronized内的加锁机制做了大量的优化，这里就是优化为CAS加锁的

你如果说在之前把ReentrantLock底层的源码都读懂了，AQS的机制都读懂了之后，那么synchronized底层的实现差不多的，synchronized的ObjectMonitor的地位就跟ReentrantLock里的AQS是差不多的

## 79、synchronized是如何使用内存屏障保证可见性和有序性的？

```java
int b = 0;
int c = 0;

synchronized(this) { -> monitorenter

Load内存屏障
Acquire内存屏障

int a = b;
c = 1; => synchronized代码块里面还是可能会发生指令重排

Release内存屏障

} -> monitorexit

Store内存屏障

```

java的并发技术底层很多都对应了内存屏障的使用，包括synchronized，他底层也是依托于各种不同的内存屏障来保证可见性和有序性的

按照可见性来划分的话，内存屏障可以分为Load屏障和Store屏障。

Load屏障的作用是执行refresh处理器缓存的操作，说白了就是对别的处理器更新过的变量，从其他处理器的高速缓存（或者主内存）加载数据到自己的高速缓存来，确保自己看到的是最新的数据。

Store屏障的作用是执行flush处理器缓存的操作，说白了就是把自己当前处理器更新的变量的值，都刷新到高速缓存（或者主内存）里去

在monitorexit指令之后，会有一个Store屏障，让线程把自己在同步代码块里修改的变量的值都执行flush处理器缓存的操作，刷到高速缓存（或者主内存）里去；然后在monitorenter指令之后会加一个Load屏障，执行refresh处理器缓存的操作，把别的处理器修改过的最新值加载到自己高速缓存里来

所以说通过Load屏障和Store屏障，就可以让synchronized保证可见性。

按照有序性保障来划分的话，还可以分为Acquire屏障和Release屏障。

在monitorenter指令之后，Load屏障之后，会加一个Acquire屏障，这个屏障的作用是禁止读操作和读写操作之间发生指令重排序。在monitorexit指令之前，会加一个Release屏障，这个屏障的作用是禁止写操作和读写操作之间发生重排序。

所以说，通过 Acquire屏障和Release屏障，就可以让synchronzied保证有序性，只有synchronized内部的指令可以重排序，但是绝对不会跟外部的指令发生重排序。

synchronized：

（1）原子性：加锁和释放锁，ObjectMonitor
（2）可见性：加了Load屏障和Store屏障，释放锁flush数据，加锁会refresh数据
（3）有序性：Acquire屏障和Release屏障，保证同步代码块内部的指令可以重排，但是同步代码块内部的指令和外面的指令是不能重排的

## 80、再看volatile关键字对原子性、可见性以及有序性的保证

volatile对原子性的保证真的是非常的有限，其实主要就是32位jvm中的long/double类型变量的赋值操作是不具备原子性的，加上volatile就可以保证原子性了

volatile boolean isRunning = true;

线程1：

Release屏障

isRunning = false;

Store屏障 => 对于之前的讲解，更进了一步，原理，没有过多的牵扯到内存屏障的一些东西，可见性和有序性，主要都是基于各种内存屏障来实现的

线程2：

Load屏障
while(isRunning) {
Acquire屏障
// 代码逻辑
}

在volatile变量写操作的前面会加入一个Release屏障，然后在之后会加入一个Store屏障，这样就可以保证volatile写跟Release屏障之前的任何读写操作都不会指令重排，然后Store屏障保证了，写完数据之后，立马会执行flush处理器缓存的操作

在volatile变量读操作的前面会加入一个Load屏障，这样就可以保证对这个变量的读取时，如果被别的处理器修改过了，必须得从其他处理器的高速缓存（或者主内存）中加载到自己本地高速缓存里，保证读到的是最新数据；在之后会加入一个Acquire屏障，禁止volatile读操作之后的任何读写操作会跟volatile读指令重排序

跟之前讲解的volatie读写内存屏障的知识对比一下，其实你看一下是类似的意思的

那个Acquire屏障其实就是LoadLoad屏障 + LoadStore屏障，Release屏障其实就是StoreLoad屏障 + StoreStore屏障

好像有点不太一样，对吧？

其实不要对内存屏障这个东西太较真，因为说句实话，不同版本的JVM，不同的底层硬件，都可能会导致加的内存屏障有一些区别，所以这个本来就没完全一致的。你只要知道内存屏障是如何保证volatile的可见性和有序性的就可以了

看各种并发相关的书和文章，对内存屏障到底是加的什么屏障，莫衷一是，没有任何一个官方权威的说法，因为这个内存屏障太底层了，底层到了涉及到了硬件，硬件不同对内存屏障的实现是不一样的

内存屏障这个东西，大概来说，其实就是大概的给你说一下这个意思，尤其是Release屏障，Store屏障和Load屏障还好理解一些，比较简单，Acqurie屏障，莫衷一是，我也没法给你一个官方的定论

具体底层的硬件实现

如果你一定 要杠到底，到底加的准确的屏障是什么？到底是如何跟上下的指令避免重排的，你自己去研究吧。我之前看过很多的资料，做过很多的研究，硬件对这个东西的实现和承诺，莫衷一是，没有标准和官方定论。

两点：volatile读写前后会加屏障，避免跟前后的读写操作发生指令重排

volatile和synchronized保证可见性和有序性，原来都是通过各种内存屏障来实现的，因为加了内存屏障，就会有一些特殊的指令和实现，就可以保证可见性和有序性了，有序性在几个阶段的指令重排的问题

内存屏障对应的底层的一些基本的硬件级别的原理，也都讲清楚了

## 81-82、高速缓存的数据结构：拉链散列表、缓存条目以及地址解码（上 下）

如果这周的课不讲，只是靠着之前的课，volatile和synchronized的原理，也能说，但是说的比较浅层一些，主要是从基础的层面来聊一下他的原理，底层的细节肯定是不行的，但是这周的课讲完了

volatile和synchronized

原子性、可见性和有序性三个方面分别来聊，这两个关键字对那几个“性”的保障是通过什么来实现的。聊到他们会加哪些内存屏障，怎么加，这些内存屏障的效果，结合底层硬件层面的一个初步的原理，来给面试官聊一下。

还是有一些遗憾的，内存屏障在硬件层面的实现的原理，到底是怎么回事，能不能再细一点，再深入一些，让大家在面试的时候聊到volatile和synchronized，直接震慑式的回答。硬件层面的一些原理

MESI协议在硬件层面的实现机制，光靠初步的MESI协议是无法保证可见性和有序性的

内存屏障在硬件层面的细致的原理，到底是如何控制那些硬件的交互和行为，最终实现可见性和有序性的保障的

volatile和synchronized，底层，彻底通透

synchronized的一些JVM对锁的优化，讲解一下；CAS底层其实也是要靠这套硬件级别的原理来给说清楚，compareAndSwap操作到底是如何在底层实现原子性的，这个东西我之前也没讲

ThreadLocal，源码基本；ReentrantLock，读写锁，源码级别

处理器高速缓存的底层数据结构实际是一个拉链散列表的结构，就是有很多个bucket，每个bucket挂了很多的cache entry，每个cache entry由三个部分组成：tag、cache line和flag，其中的cache line就是缓存的数据

tag指向了这个缓存数据在主内存中的数据的地址，flag标识了缓存行的状态，另外要注意的一点是，cache line中可以包含多个变量的值

处理器会操作一些变量，怎么在高速缓存里定位到这个变量呢？

那么处理器在读写高速缓存的时候，实际上会根据变量名执行一个内存地址解码的操作，解析出来3个东西，index、tag和offset。index用于定位到拉链散列表中的某个bucket，tag是用于定位cache entry，offset是用于定位一个变量在cache line中的位置

如果说可以成功定位到一个高速缓存中的数据，而且flag还标志着有效，则缓存命中；否则不满足上述条件，就是缓存未命中。如果是读数据未命中的话，会从主内存重新加载数据到高速缓存中，现在处理器一般都有三级高速缓存，L1、L2、L3，越靠前面的缓存读写速度越快。

可以参考83-84 标题中的图理解，即见下图

## 83-84、结合硬件级别的缓存数据结构深入分析缓存一致性协议（上 下）

<div align='center'><img src=./images/java-基础01/java-基础01_2021-03-04-15-43-09.png width='100%'/></div><br/>

因为有高速缓存的存在，所以就导致各个处理器可能对一个变量会在自己的高速缓存里有自己的副本，这样一个处理器修改了变量值，别的处理器是看不到的，所以就是为了这个问题引入了缓存一致性协议（MESI协议）

MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个处理器可以执行，其实也会通过排他锁的机制保证就一个处理器能写

之前说过那个cache entry的flag代表了缓存数据的状态，MESI协议中划分为：

（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用

（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已

（3）exclusive：独占的，标记为E，这个意思就是当前处理器对这个数据独占了，只有他可以有这个副本，其他的处理器都不能包含这个副本

（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache entry，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的

MESI协议规定了一组消息，就说各个处理器在操作内存数据的时候，都会往总线发送消息，而且各个处理器还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个处理器的协作

下面来详细的图解MESI协议的工作原理，处理器0读取某个变量的数据时，首先会根据index、tag和offset从高速缓存的拉链散列表读取数据，如果发现状态为I，也就是无效的，此时就会发送read消息到总线

接着主内存会返回对应的数据给处理器0，处理器0就会把数据放到高速缓存里，同时cache entry的flag状态是S

在处理器0对一个数据进行更新的时候，如果数据状态是S，则此时就需要发送一个invalidate消息到总线，尝试让其他的处理器的高速缓存的cache entry全部变为I，以获得数据的独占锁。

其他的处理器1会从总线嗅探到invalidate消息，此时就会把自己的cache entry设置为I，也就是过期掉自己本地的缓存，然后就是返回invalidate ack消息到总线，传递回处理器0，处理器0必须收到所有处理器返回的ack消息

接着处理器0就会将cache entry先设置为E，独占这条数据，在独占期间，别的处理器就不能修改数据了，因为别的处理器此时发出invalidate消息，这个处理器0是不会返回invalidate ack消息的，除非他先修改完再说

接着处理器0就是修改这条数据，接着将数据设置为M，也有可能是把数据此时强制写回到主内存中，具体看底层硬件实现

然后其他处理器此时这条数据的状态都是I了，那如果要读的话，全部都需要重新发送read消息，从主内存（或者是其他处理器）来加载，这个具体怎么实现要看底层的硬件了，都有可能的

这套机制其实就是缓存一致性在硬件缓存模型下的完整的执行原理

## 85、采用写缓冲器和无效队列优化MESI协议的实现性能

<div align='center'><img src=./images/java-基础01/java-基础01_2021-03-04-16-06-33.png width='100%'/></div><br/>

MESI协议如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了

所以为了解决这个问题，硬件层面引入了写缓冲器和无效队列

写缓冲器的作用是，一个处理器写数据的时候，直接把数据写入缓冲器，同时发送invalidate消息，然后就认为写操作完成了，接着就干别的事儿了，不会阻塞在这里。接着这个处理器如果之后收到其他处理器的ack消息之后

才会把写缓冲器中的写结果拿出来，通过对cache entry设置为E加独占锁，同时修改数据，然后设置为M

其实写缓冲器的作用，就是处理器写数据的时候直接写入缓冲器，不需要同步阻塞等待其他处理器的invalidate ack返回，这就大大提升了硬件层面的执行效率了

包括查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发

引入无效队列，就是说其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可

通过引入写缓冲器和无效队列，一个处理器要写数据的话，这个性能其实很高的，他直接写数据到写缓冲器，发送一个validate消息出去，就立马返回，执行别的操作了；其他处理器收到invalidate消息之后直接放入无效队列，立马就返回invalidate ack

## 86、硬件层面的MESI协议为何会引发有序性和可见性的问题？

MESI协议在硬件层面的原理其实大家都已经了解的很清晰了，对不对

可见性和有序性的问题

可见性：写缓冲器和无效队列导致的，写数据不一定立马写入自己的高速缓存（或者主内存），是因为可能写入了写缓冲器；读数据不一定立马从别人的高速缓存（或者主内存）刷新最新的值过来，invalidate消息在无效队列里面

有序性：

```java
（1）StoreLoad重排序

int a = 0;
int c = 1;

线程1：

a = 1;
int b = c;
```

这个很简单吧，第一个是Store，第二个是Load。但是可能处理器对store操作先写入了写缓冲器，此时这个写操作相当于没执行，然后就执行了第二行代码，第二行代码的b是局部变量，那这个操作等于是读取a的值，是load操作

这就导致好像第二行代码的load先执行了，第一行代码的store后执行

第一个store操作写到写缓冲器里去了，导致其他的线程是读不到的，看不到的，好像是第一个写操作没执行一样；第二个load操作成功的执行了

StoreLoad重排，Store先执行，Load后执行；Load先执行，Store后执行

（2）StoreStore重排序

resource = loadResource();
loaded = true;

两个写操作，但是可能第一个写操作写入了写缓冲器，然后第二个写操作是直接修改的高速缓存，这个时候不就导致了两个写操作顺序颠倒了？

诸如此类的重排序，都可能会因为MESI的机制发生

可见性问题也是一样的，写入写缓冲器之后，没刷入高速缓存，导致别人读不到；读数据的时候，可能invalidate消息在无效队列里，导致没法立马感知到过期的缓存，立马加载最新的数据

## 87、内存屏障在硬件层面的实现原理以及如何解决各种问题

可见性问题：

Store屏障 + Load屏障

如果加了Store屏障之后，就会强制性要求你对一个写操作必须阻塞等待到其他的处理器返回invalidate ack之后，对数据加锁，然后修改数据到高速缓存中，必须在写数据之后，强制执行flush操作

他的效果，要求一个写操作必须刷到高速缓存（或者主内存），不能停留在写缓冲里

如果加了Load屏障之后，在从高速缓存中读取数据的时候，如果发现无效队列里有一个invalidate消息，此时会立马强制根据那个invalidate消息把自己本地高速缓存的数据，设置为I（过期），然后就可以强制从其他处理器的高速缓存中加载最新的值了

这就是refresh操作

为了解决有序性问题

内存屏障，Acquire屏障，Release屏障，但是都是由基础的StoreStore屏障,StoreLoad屏障，可以避免指令重排序的效果

StoreStore屏障，会强制让写数据的操作全部按照顺序写入写缓冲器里，他不会让你第一个写到写缓冲器里去，第二个写直接修改高速缓存了

resource = loadResource();
StoreStore屏障
loaded = true;

StoreLoad屏障，他会强制先将写缓冲器里的数据写入高速缓存中，接着读数据的时候强制清空无效队列，对里面的validate消息全部过期掉高速缓存中的条目，然后强制从主内存里重新加载数据

a = 1; // 强制要求必须直接写入高速缓存，不能停留在写缓冲器里，清空写缓冲器里的这条数据
int b = c;

## 88、在复杂的硬件模型之上的Java内存模型是如何大幅简化的？

## 89、面试的时候如何从内存屏障、硬件层面的原理来震慑面试官

volatile、synchronized

原子性这块，直接把底层的一些东西喷出来

硬件层面的原理 -> MESI协议在硬件层面运行的原理 -> 这套原理为何会导致可见性和有序性的问题 -> 各种内存屏障是如何在硬件层面解决可见性和有序性的问题 -> volatile和synchroized是如何加各种内存屏障来分别保证可见性和有序性的

行业里对并发这块知识掌握到这个层面的人，不多

很多人写并发的书，如果你把我们的课看完了，有并发的书，你去看看，XX并发实战，书里的内容很浅，你的水平可能已经超过部分写并发书籍的作者了

是个面试官，主要不是技术特别牛的，一般的人多会被你给震慑到，从硬件层面开始画图

## 90-91、Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁（上 下）

从JDk 1.6开始，JVM就对synchronized锁进行了很多的优化

有个别同学完全没搞明白并发到底是怎么回事，一直追着问，什么是偏向锁，什么是自旋锁，锁是一种单独的锁类别。真是不懂并发技术，小白，小菜，too young too simple。其实是synchronized底层的优化和实现

synchronized说是锁，但是他的底层加锁的方式 可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁

这些东西本身你只要了解一个概念就可以了，JDK 1.6开始对synchronized关键字做过哪些优化，有哪些加锁的方式，效果是什么，作用是什么，在实际的开发和使用中，根本就不需要你去过多的care一些东西

```java
synchronized(this) {

}
```

（1）锁消除

锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令

这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁

（2）锁粗化

```java
synchronized(this) {

}

synchronized(this) {

}

synchronized(this) {

}
```

这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁

（3）偏向锁

这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS

性能会提升很多

但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好

可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小

如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好

（4）轻量级锁

如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁

如果是自己加的锁，那就执行代码就好了

如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁

（5）适应性锁

这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒

也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大

所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了

一直追问我，什么自旋锁，不是什么事儿，当然，如果要站在jvm的底层层面，去说清楚的话，确实是比较复杂的，但是我觉得起码目前为止，暂时也没必要，各种锁底层是如何来实现的，完全可以等到以后jvm那块都讲过之后

再回过头来深入jvm底层的原理来剖析：偏向锁、自旋锁、轻量级锁，jvm层面的概念，栈侦，Load Record，不一定能听懂，基础的知识没有铺垫好，需要通过调节jvm的一些参数来优化底层synchronized里的各种加锁方式的使用

这样可以大幅度减少线程上下文的切换，而这种自旋等待获取锁的方式，就是所谓自旋锁，就是不断的自旋尝试获取锁

如果一个线程持有锁的时间很长，那么其他线程获取不到锁，就会暂停，发生上下文切换，让其他线程来执行，这种自己暂停获取锁的方式，就是所谓的重量级锁

这个根据不同情况自动调整的过程，就是适应锁的意思

## 92、再来看看CAS是如何基于MESI协议在底层硬件层面实现加锁的？

无法发出指令来执行一个原子性的cas，先查出数据，比较一下，如果一样，就写数据。MESI协议有关系

volatile、synchronized（内存屏障实现）、CAS(是在硬件级别实现，来保障原子性的)、ThreadLocal、ReentrantReadWriteLock、锁优化、锁生产故障

并发的核心和关键的技术都到了硬件和源码的级别，大家都应该掌握的很好了

先讲线程安全的并发包下的集合，同步器组件，线程池，并发的核心技术，并发编程设计模式完全结合我们后续的微服务注册中心的项目完善、开发和实战来演练

